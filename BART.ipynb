{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "37eb328b-1638-41a1-bee5-87da87ce30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "1ae19c6c-289a-46b8-95fb-cbcacb49b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#print(os.getcwd())       # Confirm folder\n",
    "#print(os.listdir())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3de7f",
   "metadata": {},
   "source": [
    "## Importing and cleaning data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "077561f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_path = \"section_train_0.parquet\" #file path\n",
    "section = pd.read_parquet(section_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "b69f989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documention_path_0 = \"document_train_0.parquet\" #file path\n",
    "document_0 = pd.read_parquet(documention_path_0)\n",
    "\n",
    "documention_path_1 = \"document_train_1.parquet\"\n",
    "document_1 = pd.read_parquet(documention_path_1)\n",
    "\n",
    "documention_path_2 = \"document_train_2.parquet\"\n",
    "document_2 = pd.read_parquet(documention_path_2)\n",
    "\n",
    "documention_path_3 = \"document_train_3.parquet\"\n",
    "document_3 = pd.read_parquet(documention_path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "f4213b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = pd.concat([document_0, document_1, document_2, document_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "e3e4145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "document['abstract'] = document['abstract'].str.replace(' \\n', '', regex=False) #remove \\n\n",
    "document['abstract'] = document['abstract'].str.replace(r'@\\w+', '', regex=True) # remove @ formulas\n",
    "document['abstract'] = document['abstract'].str.replace('  ', ' ', regex=False) #replace double space with single space\n",
    "\n",
    "document['article'] = document['article'].str.replace(r'@\\w+', '', regex=True) # remove @ formulas\n",
    "document['article'] = document['article'].str.replace('  ', ' ', regex=False) #replace double space with single space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea992138",
   "metadata": {},
   "source": [
    "## Remove samples with missing info and samples with too much noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "f93ef112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     54144.000000\n",
       "mean      30196.147292\n",
       "std       22630.985389\n",
       "min           0.000000\n",
       "25%       16117.500000\n",
       "50%       24903.000000\n",
       "75%       38290.500000\n",
       "max      607645.000000\n",
       "Name: article, dtype: float64"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_summary = document['article'].str.len().describe()\n",
    "article_summary['25%']\n",
    "article_summary['75%']\n",
    "\n",
    "article_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "7764ee3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    54144.000000\n",
       "mean      1490.463542\n",
       "std       2872.869021\n",
       "min          7.000000\n",
       "25%        659.000000\n",
       "50%        938.000000\n",
       "75%       1324.000000\n",
       "max      69493.000000\n",
       "Name: abstract, dtype: float64"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_summary = document['abstract'].str.len().describe()\n",
    "abstract_summary['25%']\n",
    "abstract_summary['75%']\n",
    "\n",
    "abstract_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "e29ce969",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = document[document['article'].str.len() >= article_summary['25%']] \n",
    "document = document[document['article'].str.len() <= article_summary['75%']]  \n",
    "\n",
    "document = document[document['abstract'].str.len() >= abstract_summary['25%']]\n",
    "document = document[document['abstract'].str.len() <= abstract_summary['75%']]\n",
    "\n",
    "document = document.drop_duplicates()\n",
    "document = document.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "5587253e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>abstract</th>\n",
       "      <th>article_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>the luminous blue variable ( lbv ) phase is be...</td>\n",
       "      <td>the paradigmatic luminous blue variable r127 i...</td>\n",
       "      <td>16118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9703</th>\n",
       "      <td>although the discovery of `` a higgs - like bo...</td>\n",
       "      <td>we consider the most general set of invariant ...</td>\n",
       "      <td>16120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15248</th>\n",
       "      <td>the formulation of gauge theories on discrete ...</td>\n",
       "      <td>the phase diagram of lattice gauge theory is i...</td>\n",
       "      <td>16122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12159</th>\n",
       "      <td>magnetic fields can greatly influence stellar ...</td>\n",
       "      <td>we present 2d mhd simulations of the radiative...</td>\n",
       "      <td>16124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13160</th>\n",
       "      <td>rare b decays are mediated by flavor changing ...</td>\n",
       "      <td>we calculate the zeroes of angular observables...</td>\n",
       "      <td>16126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6870</th>\n",
       "      <td>for more than two decades astrophysicists have...</td>\n",
       "      <td>we present results from the first and simulati...</td>\n",
       "      <td>38270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6218</th>\n",
       "      <td>vortex arrays ( va s ) in type ii superconduct...</td>\n",
       "      <td>the flow properties of confined vortex matter ...</td>\n",
       "      <td>38271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13153</th>\n",
       "      <td>networked systems , such as social , biologica...</td>\n",
       "      <td>we consider three distinct and well studied pr...</td>\n",
       "      <td>38272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11051</th>\n",
       "      <td>a wireless relay network is one in which a set...</td>\n",
       "      <td>in this paper , a cooperative transmission des...</td>\n",
       "      <td>38274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>globular and open clusters provide useful prob...</td>\n",
       "      <td>we prove the existence of the old and metal - ...</td>\n",
       "      <td>38286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15490 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "3651   the luminous blue variable ( lbv ) phase is be...   \n",
       "9703   although the discovery of `` a higgs - like bo...   \n",
       "15248  the formulation of gauge theories on discrete ...   \n",
       "12159  magnetic fields can greatly influence stellar ...   \n",
       "13160  rare b decays are mediated by flavor changing ...   \n",
       "...                                                  ...   \n",
       "6870   for more than two decades astrophysicists have...   \n",
       "6218   vortex arrays ( va s ) in type ii superconduct...   \n",
       "13153  networked systems , such as social , biologica...   \n",
       "11051  a wireless relay network is one in which a set...   \n",
       "6265   globular and open clusters provide useful prob...   \n",
       "\n",
       "                                                abstract  article_length  \n",
       "3651   the paradigmatic luminous blue variable r127 i...           16118  \n",
       "9703   we consider the most general set of invariant ...           16120  \n",
       "15248  the phase diagram of lattice gauge theory is i...           16122  \n",
       "12159  we present 2d mhd simulations of the radiative...           16124  \n",
       "13160  we calculate the zeroes of angular observables...           16126  \n",
       "...                                                  ...             ...  \n",
       "6870   we present results from the first and simulati...           38270  \n",
       "6218   the flow properties of confined vortex matter ...           38271  \n",
       "13153  we consider three distinct and well studied pr...           38272  \n",
       "11051  in this paper , a cooperative transmission des...           38274  \n",
       "6265   we prove the existence of the old and metal - ...           38286  \n",
       "\n",
       "[15490 rows x 3 columns]"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_sorted = (\n",
    "    document\n",
    "    .assign(article_length=document['article'].str.len())\n",
    "    .sort_values(by='article_length', ascending=True)\n",
    ")\n",
    "\n",
    "document_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "91769f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstract_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9484</th>\n",
       "      <td>half - metallicity is the property of some spi...</td>\n",
       "      <td>we report on first - principles calculations o...</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8298</th>\n",
       "      <td>one of the important tools to study the proper...</td>\n",
       "      <td>the paper presents analysis of the single top ...</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14987</th>\n",
       "      <td>the growing flux of partons at high energy wil...</td>\n",
       "      <td>at the lhc multiple parton interactions will r...</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6839</th>\n",
       "      <td>is a nearby ( distance to the earth ) young m...</td>\n",
       "      <td>the red spectral shape of the visible to near ...</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4328</th>\n",
       "      <td>solar flares have been historically divided in...</td>\n",
       "      <td>gamma - ray production cross sections have bee...</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8901</th>\n",
       "      <td>it has long been recognized that by studying t...</td>\n",
       "      <td>the magic collaboration has recently reported ...</td>\n",
       "      <td>1324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>the nature of unidentified high - energy -ray ...</td>\n",
       "      <td>we report the detection of -ray pulsations ( g...</td>\n",
       "      <td>1324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6305</th>\n",
       "      <td>the liberalised electricity market poses new c...</td>\n",
       "      <td>hydro storage system optimization is becoming ...</td>\n",
       "      <td>1324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6352</th>\n",
       "      <td>many of the ideas presented in this paper have...</td>\n",
       "      <td>the path integral by which quantum field theor...</td>\n",
       "      <td>1324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13707</th>\n",
       "      <td>interfaces and surfaces are present in practic...</td>\n",
       "      <td>we investigate the chemical composition and ad...</td>\n",
       "      <td>1324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15490 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "9484   half - metallicity is the property of some spi...   \n",
       "8298   one of the important tools to study the proper...   \n",
       "14987  the growing flux of partons at high energy wil...   \n",
       "6839    is a nearby ( distance to the earth ) young m...   \n",
       "4328   solar flares have been historically divided in...   \n",
       "...                                                  ...   \n",
       "8901   it has long been recognized that by studying t...   \n",
       "1293   the nature of unidentified high - energy -ray ...   \n",
       "6305   the liberalised electricity market poses new c...   \n",
       "6352   many of the ideas presented in this paper have...   \n",
       "13707  interfaces and surfaces are present in practic...   \n",
       "\n",
       "                                                abstract  abstract_length  \n",
       "9484   we report on first - principles calculations o...              659  \n",
       "8298   the paper presents analysis of the single top ...              659  \n",
       "14987  at the lhc multiple parton interactions will r...              659  \n",
       "6839   the red spectral shape of the visible to near ...              659  \n",
       "4328   gamma - ray production cross sections have bee...              659  \n",
       "...                                                  ...              ...  \n",
       "8901   the magic collaboration has recently reported ...             1324  \n",
       "1293   we report the detection of -ray pulsations ( g...             1324  \n",
       "6305   hydro storage system optimization is becoming ...             1324  \n",
       "6352   the path integral by which quantum field theor...             1324  \n",
       "13707  we investigate the chemical composition and ad...             1324  \n",
       "\n",
       "[15490 rows x 3 columns]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_sorted = (\n",
    "    document\n",
    "    .assign(abstract_length=document['abstract'].str.len())\n",
    "    .sort_values(by='abstract_length', ascending=True)\n",
    ")\n",
    "\n",
    "document_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "b136e703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'additive models provide an important family of models for semiparametric regression or classification . some reasons for the success of additive models are their increased flexibility when compared to linear or generalized linear models and their increased interpretability when compared to fully nonparametric models . it is well - known that good estimators in additive models are in general less prone to the curse of high dimensionality than good estimators in fully nonparametric models . many examples of such estimators belong to the large class of regularized kernel based methods over a reproducing kernel hilbert space , see e.g. . in the last years many interesting results on learning rates of regularized kernel based models for additive models have been published when the focus is on sparsity and when the classical least squares loss function is used , see e.g. , , , , , and the references therein . of course , the least squares loss function is differentiable and has many nice mathematical properties , but it is only locally lipschitz continuous and therefore regularized kernel based methods based on this loss function typically suffer on bad statistical robustness properties , even if the kernel is bounded . this is in sharp contrast to kernel methods based on a lipschitz continuous loss function and on a bounded loss function , where results on upper bounds for the maxbias bias and on a bounded influence function are known , see e.g. for the general case and for additive models . therefore , we will here consider the case of regularized kernel based methods based on a general convex and lipschitz continuous loss function , on a general kernel , and on the classical regularizing term for some which is a smoothness penalty but not a sparsity penalty , see e.g. . such regularized kernel based methods are now often called support vector machines ( svms ) , although the notation was historically used for such methods based on the special hinge loss function and for special kernels only , we refer to .  in this paper we address the open question , whether an svm with an additive kernel can provide a substantially better learning rate in high dimensions than an svm with a general kernel , say a classical gaussian rbf kernel , if the assumption of an additive model is satisfied . our leading example covers learning rates for quantile regression based on the lipschitz continuous but non - differentiable pinball loss function , which is also called check function in the literature , see e.g. and for parametric quantile regression and , , and for kernel based quantile regression . we will not address the question how to check whether the assumption of an additive model is satisfied because this would be a topic of a paper of its own . of course , a practical approach might be to fit both models and compare their risks evaluated for test data . for the same reason we will also not cover sparsity . consistency of support vector machines generated by additive kernels for additive models was considered in . in this paper we establish learning rates for these algorithms . let us recall the framework with a complete separable metric space as the input space and a closed subset of as the output space . a borel probability measure on is used to model the learning problem and an independent and identically distributed sample is drawn according to for learning . a loss function is used to measure the quality of a prediction function by the local error . _ throughout the paper we assume that is measurable , , convex with respect to the third variable , and uniformly lipschitz continuous satisfying with a finite constant . _  support vector machines ( svms ) considered here are kernel - based regularization schemes in a reproducing kernel hilbert space ( rkhs ) generated by a mercer kernel . with a shifted loss function introduced for dealing even with heavy - tailed distributions as , they take the form where for a general borel measure on , the function is defined by where is a regularization parameter . the idea to shift a loss function has a long history , see e.g. in the context of m - estimators . it was shown in that is also a minimizer of the following optimization problem involving the original loss function if a minimizer exists :   the additive model we consider consists of the _ input space decomposition _ with each a complete separable metric space and a _ hypothesis space _ where is a set of functions each of which is also identified as a map from to . hence the functions from take the additive form . we mention , that there is strictly speaking a notational problem here , because in the previous formula each quantity is an element of the set which is a subset of the full input space , , whereas in the definition of sample each quantity is an element of the full input space , where . because these notations will only be used in different places and because we do not expect any misunderstandings , we think this notation is easier and more intuitive than specifying these quantities with different symbols . the additive kernel is defined in terms of mercer kernels on as it generates an rkhs which can be written in terms of the rkhs generated by on corresponding to the form ( [ additive ] ) as with norm given by the norm of satisfies   to illustrate advantages of additive models , we provide two examples of comparing additive with product kernels . the first example deals with gaussian rbf kernels . all proofs will be given in section [ proofsection ] . [ gaussadd ] let , $ ] and ^ 2.$ ] let and .\\\\ ] ] the additive kernel is given by furthermore , the product kernel is the standard gaussian kernel given by define a gaussian function on ^ 2 $ ] depending only on one variable by then but where denotes the rkhs generated by the standard gaussian rbf kernel . the second example is about sobolev kernels . [ sobolvadd ] let , $ ] and ^s.$ ] let : = \\\\bigl\\\\{u\\\\in l_2([0,1 ] ) ; d^\\\\alpha u \\\\in l_2([0,1 ] ) \\\\mbox{~for~all~}|\\\\alpha|\\\\le 1\\\\bigr\\\\}\\\\ ] ] be the sobolev space consisting of all square integrable univariate functions whose derivative is also square integrable . it is an rkhs with a mercer kernel defined on ^ 2 $ ] . if we take all the mercer kernels to be , then $ ] for each . the additive kernel is also a mercer kernel and defines an rkhs \\\\right\\\\}.\\\\ ] ] however , the multivariate sobolev space ^s)$ ] , consisting of all square integrable functions whose partial derivatives are all square integrable , contains discontinuous functions and is not an rkhs . denote the marginal distribution of on as . under the assumption that for each and that is dense in in the -metric , it was proved in that in probability as long as satisfies and . the rest of the paper has the following structure . section [ ratessection ] contains our main results on learning rates for svms based on additive kernels . learning rates for quantile regression are treated as important special cases . section [ comparisonsection ] contains a comparison of our results with other learning rates published recently . section [ proofsection ] contains all the proofs and some results which can be interesting in their own . in this paper we provide some learning rates for the support vector machines generated by additive kernels for additive models which helps improve the quantitative understanding presented in . the rates are about asymptotic behaviors of the excess risk and take the form with . they will be stated under three kinds of conditions involving the hypothesis space , the measure , the loss , and the choice of the regularization parameter . the first condition is about the approximation ability of the hypothesis space . since the output function is from the hypothesis space , the learning rates of the learning algorithm depend on the approximation ability of the hypothesis space with respect to the optimal risk measured by the following approximation error . [ defapprox ] the approximation error of the triple is defined as   to estimate the approximation error , we make an assumption about the minimizer of the risk   for each , define the integral operator associated with the kernel by we mention that is a compact and positive operator on . hence we can find its normalized eigenpairs such that is an orthonormal basis of and as . fix . then we can define the -th power of by this is a positive and bounded operator and its range is well - defined . the assumption means lies in this range . [ assumption1 ] we assume and where for some and each , is a function of the form with some . the case of assumption [ assumption1 ] means each lies in the rkhs . a standard condition in the literature ( e.g. , ) for achieving decays of the form for the approximation error ( [ approxerrordef ] ) is with some . here the operator is defined by in general , this can not be written in an additive form . however , the hypothesis space ( [ additive ] ) takes an additive form . so it is natural for us to impose an additive expression for the target function with the component functions satisfying the power condition . the above natural assumption leads to a technical difficulty in estimating the approximation error : the function has no direct connection to the marginal distribution projected onto , hence existing methods in the literature ( e.g. , ) can not be applied directly . note that on the product space , there is no natural probability measure projected from , and the risk on is not defined .  our idea to overcome the difficulty is to introduce an intermediate function . it may not minimize a risk ( which is not even defined ) . however , it approximates the component function well . when we add up such functions , we get a good approximation of the target function , and thereby a good estimate of the approximation error . this is the first novelty of the paper . [ approxerrorthm ] under assumption [ assumption1 ] , we have where is the constant given by    the second condition for our learning rates is about the capacity of the hypothesis space measured by -empirical covering numbers .  let be a set of functions on and for every the * covering number of * with respect to the empirical metric , given by is defined as and the * -empirical covering number * of is defined as   [ assumption2 ] we assume and that for some , and every , the -empirical covering number of the unit ball of satisfies   the second novelty of this paper is to observe that the additive nature of the hypothesis space yields the following nice bound with a dimension - independent power exponent for the covering numbers of the balls of the hypothesis space , to be proved in section [ samplesection ] . [ capacitythm ] under assumption [ assumption2 ] , for any and , we have   the bound for the covering numbers stated in theorem [ capacitythm ] is special : the power is independent of the number of the components in the additive model . it is well - known in the literature of function spaces that the covering numbers of balls of the sobolev space on the cube ^s$ ] of the euclidean space with regularity index has the following asymptotic behavior with : here the power depends linearly on the dimension . similar dimension - dependent bounds for the covering numbers of the rkhss associated with gaussian rbf - kernels can be found in . the special bound in theorem [ capacitythm ] demonstrates an advantage of the additive model in terms of capacity of the additive hypothesis space . the third condition for our learning rates is about the noise level in the measure with respect to the hypothesis space . before stating the general condition , we consider a special case for quantile regression , to illustrate our general results . let be a quantile parameter . the quantile regression function is defined by its value to be a -quantile of , i.e. , a value satisfying the regularization scheme for quantile regression considered here takes the form ( [ algor ] ) with the loss function given by the pinball loss as   a noise condition on for quantile regression is defined in as follows . to this end , let be a probability measure on and . then a real number is called -quantile of , if and only if belongs to the set \\\\bigr ) \\\\ge \\\\tau   \\\\mbox{~~and~~ } q\\\\bigl([t , \\\\infty)\\\\bigr ) \\\\ge 1-\\\\tau\\\\bigr\\\\}\\\\,.\\\\ ] ] it is well - known that is a compact interval . [ noisecond ] let .  1 . a probability measure on is said to have a * -quantile of type * , if there exist a -quantile and a constant such that , for all $ ] , we have 2 . let $ ] . we say that a probability measure on has a * -quantile of -average type * if the conditional probability measure has -almost surely a -quantile of type and the function where is the constant defined in part ( 1 ) , satisfies . one can show that a distribution having a -quantile of type has a unique -quantile . moreover , if has a lebesgue density then has a -quantile of type if is bounded away from zero on $ ] since we can use \\\\}$ ] in ( [ tauquantileoftype2formula ] ) . this assumption is general enough to cover many distributions used in parametric statistics such as gaussian , student s , and logistic distributions ( with ) , gamma and log - normal distributions ( with ) , and uniform and beta distributions ( with $ ] ) . the following theorem , to be proved in section [ proofsection ] , gives a learning rate for the regularization scheme ( [ algor ] ) in the special case of quantile regression . [ quantilethm ] suppose that almost surely for some constant , and that each kernel is with for some . if assumption [ assumption1 ] holds with and has a -quantile of -average type for some $ ] , then by taking , for any and , with confidence at least we have where is a constant independent of and and   please note that the exponent given by ( [ quantilerates2 ] ) for the learning rate in ( [ quantilerates ] ) is independent of the quantile level , of the number of additive components in , and of the dimensions and further note that , if , and if . because can be arbitrarily close to , the learning rate , which is independent of the dimension and given by theorem [ quantilethm ] , is close to for large values of and is close to or better , if .   to state our general learning rates , we need an assumption on a _ variance - expectation bound _ which is similar to definition [ noisecond ] in the special case of quantile regression . [ assumption3 ] we assume that there exist an exponent $ ] and a positive constant such that   assumption [ assumption3 ] always holds true for . if the triple satisfies some conditions , the exponent can be larger . for example , when is the pinball loss ( [ pinloss ] ) and has a -quantile of -average type for some $ ] and as defined in , then . [ mainratesthm ] suppose that is bounded by a constant almost surely . under assumptions [ assumption1 ] to [ assumption3 ] , if we take and for some , then for any , with confidence at least we have where is given by and is constant independent of or ( to be given explicitly in the proof ) . we now add some theoretical and numerical comparisons on the goodness of our learning rates with those from the literature . as already mentioned in the introduction , some reasons for the popularity of additive models are flexibility , increased interpretability , and ( often ) a reduced proneness of the curse of high dimensions . hence it is important to check , whether the learning rate given in theorem [ mainratesthm ] under the assumption of an additive model favourably compares to ( essentially ) optimal learning rates without this assumption . in other words , we need to demonstrate that the main goal of this paper is achieved by theorem [ quantilethm ] and theorem [ mainratesthm ] , i.e. that an svm based on an additive kernel can provide a substantially better learning rate in high dimensions than an svm with a general kernel , say a classical gaussian rbf kernel , provided the assumption of an additive model is satisfied . our learning rate in theorem [ quantilethm ] is new and optimal in the literature of svm for quantile regression . most learning rates in the literature of svm for quantile regression are given for projected output functions , while it is well known that projections improve learning rates . here the projection operator is defined for any measurable function by sometimes this is called clipping . such results are given in . for example , under the assumptions that has a -quantile of -average type , the approximation error condition ( [ approxerrorb ] ) is satisfied for some , and that for some constants , the sequence of eigenvalues of the integral operator satisfies for every , it was shown in that with confidence at least , where here the parameter measures the capacity of the rkhs and it plays a similar role as half of the parameter in assumption 2 . for a kernel and , one can choose and to be arbitrarily small and the above power index can be taken as . the learning rate in theorem [ quantilethm ] may be improved by relaxing assumption 1 to a sobolev smoothness condition for and a regularity condition for the marginal distribution . for example , one may use a gaussian kernel depending on the sample size and achieve the approximation error condition ( [ approxerrorb ] ) for some . this is done for quantile regression in . since we are mainly interested in additive models , we shall not discuss such an extension . [ gaussmore ] let , $ ] and ^ 2.$ ] let and the additive kernel be given by ( [ gaussaddform ] ) with in example [ gaussadd ] as .\\\\ ] ] if the function is given by ( [ gaussfcn ] ) , almost surely for some constant , and has a -quantile of -average type for some $ ] , then by taking , for any and , ( [ quantilerates ] ) holds with confidence at least .  it is unknown whether the above learning rate can be derived by existing approaches in the literature ( e.g. ) even after projection . note that the kernel in the above example is independent of the sample size . it would be interesting to see whether there exists some such that the function defined by ( [ gaussfcn ] ) lies in the range of the operator . the existence of such a positive index would lead to the approximation error condition ( [ approxerrorb ] ) , see .  let us now add some numerical comparisons on the goodness of our learning rates given by theorem [ mainratesthm ] with those given by . their corollary 4.12 gives ( essentially ) minmax optimal learning rates for ( clipped ) svms in the context of nonparametric quantile regression using one gaussian rbf kernel on the whole input space under appropriate smoothness assumptions of the target function . let us consider the case that the distribution has a -quantile of -average type , where , and assume that both corollary 4.12 in and our theorem [ mainratesthm ] are applicable . i.e. , we assume in particular that is a probability measure on $ ] and that the marginal distribution has a lebesgue density for some . furthermore , suppose that the optimal decision function has ( to make theorem [ mainratesthm ] applicable with $ ] ) the additive structure with each as stated in assumption [ assumption1 ] , where and , with minimal risk and additionally fulfills ( to make corollary 4.12 in applicable ) where $ ] and denotes a besov space with smoothness parameter . the intuitive meaning of is , that increasing values of correspond to increased smoothness . we refer to ( * ? ? ? * and p. 44 ) for details on besov spaces . it is well - known that the besov space contains the sobolev space for , , and , and that . we mention that if all are suitably chosen wendland kernels , their reproducing kernel hilbert spaces are sobolev spaces , see ( * ? ? ? * thm . 10.35 , p. 160 ) . furthermore , we use the same sequence of regularizing parameters as in ( * ? ? ? 4.9 , cor . 4.12 ) , i.e. , where , , $ ] , and is some user - defined positive constant independent of . for reasons of simplicity , let us fix . then ( * ? ? ? 4.12 ) gives learning rates for the risk of svms for -quantile regression , if a single gaussian rbf - kernel on is used for -quantile functions of -average type with , which are of order hence the learning rate in theorem [ quantilethm ] is better than the one in ( * ? ? ? 4.12 ) in this situation , if provided the assumption of the additive model is valid . table [ table1 ] lists the values of from ( [ explicitratescz2 ] ) for some finite values of the dimension , where . all of these values of are positive with the exceptions if or . this is in contrast to the corresponding exponent in the learning rate by ( * ? ? * cor . 4.12 ) , because   table [ table2 ] and figures [ figure1 ] to [ figure2 ] give additional information on the limit . of course , higher values of the exponent indicates faster rates of convergence . it is obvious , that an svm based on an additive kernel has a significantly faster rate of convergence in higher dimensions compared to svm based on a single gaussian rbf kernel defined on the whole input space , of course under the assumption that the additive model is valid . the figures seem to indicate that our learning rate from theorem [ mainratesthm ] is probably not optimal for small dimensions . however , the main focus of the present paper is on high dimensions . .[table1 ] the table lists the limits of the exponents from ( * ? ? ? * cor . 4.12 ) and from theorem [ mainratesthm ] , respectively , if the regularizing parameter is chosen in an optimal manner for the nonparametric setup , i.e. , with for and . recall that $ ] . [ cols= \" > , > , > , > \" , ]'"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document['article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "80867f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'additive models play an important role in semiparametric statistics . this paper gives learning rates for regularized kernel based methods for additive models . these learning rates compare favourably in particular in high dimensions to recent results on optimal learning rates for purely nonparametric regularized kernel based quantile regression using the gaussian radial basis function kernel , provided the assumption of an additive model is valid . additionally , a concrete example is presented to show that a gaussian function depending only on one variable lies in a reproducing kernel hilbert space generated by an additive gaussian kernel , but does not belong to the reproducing kernel hilbert space generated by the multivariate gaussian kernel of the same variance .  * key words and phrases . * additive model , kernel , quantile regression , semiparametric , rate of convergence , support vector machine .'"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document['abstract'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde51bd0",
   "metadata": {},
   "source": [
    "## BART (No training yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "ee1a453c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 511/511 [00:00<00:00, 1004.17it/s, Materializing param=model.encoder.layers.11.self_attn_layer_norm.weight]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, output_attentions=True)\n",
    "\n",
    "model.config.output_attentions = True\n",
    "model.config.return_dict = True\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "3f34c3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additive models provide an important family of models for semiparametric regression or classification. Some reasons for the success of additive models are their increased flexibility when compared to linear or generalized linear models and their increased interpretability. In this paper we address the open question whether an svm with an additive kernel can provide a substantially better learning rate in high dimensions. We will not address the question how to check whether the assumption of an additive model is satisfied because this would be a topic of a paper of its own.\n"
     ]
    }
   ],
   "source": [
    "text = document['article'][0]\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "generated = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    num_beams=4,\n",
    "    max_length=500,\n",
    "    min_length=100,\n",
    "    #length_penalty=2.0, # can change min and max length as well as this to see if we want to have the summary be more brief     \n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=3,\n",
    "    do_sample=False,\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "\n",
    "generated_ids = generated.sequences\n",
    "\n",
    "summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(summary)\n",
    "\n",
    "#for attention values\n",
    "outputs = model(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    decoder_input_ids=generated_ids,\n",
    "    output_attentions=True,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "cross_attentions = outputs.cross_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "0efbebff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'additive models play an important role in semiparametric statistics . this paper gives learning rates for regularized kernel based methods for additive models . these learning rates compare favourably in particular in high dimensions to recent results on optimal learning rates for purely nonparametric regularized kernel based quantile regression using the gaussian radial basis function kernel , provided the assumption of an additive model is valid . additionally , a concrete example is presented to show that a gaussian function depending only on one variable lies in a reproducing kernel hilbert space generated by an additive gaussian kernel , but does not belong to the reproducing kernel hilbert space generated by the multivariate gaussian kernel of the same variance .  * key words and phrases . * additive model , kernel , quantile regression , semiparametric , rate of convergence , support vector machine .'"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document['abstract'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "b3223531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "torch.Size([1, 16, 105, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(len(cross_attentions))\n",
    "print(cross_attentions[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "a9ff3d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[1.9530e-01, 6.6491e-07, 8.6813e-10,  ..., 4.6027e-08,\n",
       "            1.8740e-08, 3.3442e-02],\n",
       "           [7.8566e-02, 3.6558e-05, 1.2620e-08,  ..., 3.2075e-08,\n",
       "            1.8732e-09, 4.3937e-02],\n",
       "           [1.4969e-03, 9.6271e-01, 2.0434e-06,  ..., 9.9358e-10,\n",
       "            1.5781e-08, 1.2946e-03],\n",
       "           ...,\n",
       "           [2.9099e-03, 1.7626e-07, 4.7626e-07,  ..., 3.8807e-09,\n",
       "            3.5607e-07, 9.4704e-04],\n",
       "           [1.6940e-01, 9.5440e-07, 6.9736e-07,  ..., 5.7952e-09,\n",
       "            1.4324e-08, 6.0313e-02],\n",
       "           [7.8984e-02, 1.0426e-08, 5.9409e-11,  ..., 3.7038e-09,\n",
       "            1.4941e-07, 8.5193e-02]],\n",
       " \n",
       "          [[7.5060e-02, 1.1381e-04, 1.7044e-04,  ..., 1.7028e-04,\n",
       "            1.5922e-04, 3.0086e-02],\n",
       "           [8.7623e-02, 6.6379e-04, 3.1574e-04,  ..., 4.5471e-05,\n",
       "            4.4424e-05, 2.2373e-02],\n",
       "           [2.5617e-01, 1.4498e-03, 4.5393e-04,  ..., 5.4525e-05,\n",
       "            8.1151e-05, 2.6895e-02],\n",
       "           ...,\n",
       "           [2.5342e-01, 7.4780e-04, 4.3976e-04,  ..., 2.0800e-04,\n",
       "            1.4122e-04, 3.9974e-02],\n",
       "           [1.9577e-01, 4.7516e-04, 3.1104e-04,  ..., 1.9760e-05,\n",
       "            1.0769e-05, 2.9712e-02],\n",
       "           [5.6435e-02, 2.3952e-03, 7.9248e-04,  ..., 6.1727e-05,\n",
       "            5.8592e-05, 1.7348e-02]],\n",
       " \n",
       "          [[9.3938e-02, 1.0555e-03, 7.5077e-04,  ..., 1.3038e-04,\n",
       "            1.7910e-04, 1.0925e-01],\n",
       "           [7.7801e-02, 1.3454e-02, 1.2855e-03,  ..., 3.8873e-05,\n",
       "            4.6689e-05, 4.4522e-02],\n",
       "           [6.2940e-02, 6.4286e-02, 2.3435e-03,  ..., 4.9867e-05,\n",
       "            1.2251e-05, 1.5143e-01],\n",
       "           ...,\n",
       "           [3.3694e-01, 9.1070e-04, 5.9563e-04,  ..., 9.2075e-05,\n",
       "            8.1678e-05, 2.5907e-02],\n",
       "           [2.5798e-01, 1.4465e-05, 2.8417e-05,  ..., 5.2632e-06,\n",
       "            1.4553e-06, 3.2249e-03],\n",
       "           [1.4376e-01, 4.9201e-03, 1.5625e-03,  ..., 1.8476e-04,\n",
       "            1.4628e-04, 4.2315e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[3.8571e-02, 8.9348e-05, 1.2877e-05,  ..., 5.1119e-07,\n",
       "            8.6516e-07, 9.5180e-01],\n",
       "           [3.7442e-02, 8.0704e-04, 3.9497e-05,  ..., 4.8398e-07,\n",
       "            6.8139e-07, 9.4440e-01],\n",
       "           [1.8269e-01, 2.6960e-03, 4.6926e-04,  ..., 8.6935e-06,\n",
       "            2.0406e-05, 1.2573e-02],\n",
       "           ...,\n",
       "           [5.8844e-04, 4.8403e-04, 1.0643e-03,  ..., 1.8557e-04,\n",
       "            8.9640e-05, 2.2900e-04],\n",
       "           [3.4967e-06, 6.3323e-03, 1.9245e-04,  ..., 5.0009e-04,\n",
       "            2.4119e-04, 4.8535e-05],\n",
       "           [2.3161e-04, 1.3025e-02, 8.8536e-04,  ..., 3.8170e-05,\n",
       "            2.7544e-05, 8.8938e-04]],\n",
       " \n",
       "          [[9.5807e-03, 1.7259e-03, 5.1993e-05,  ..., 3.9936e-04,\n",
       "            2.8333e-04, 1.0584e-02],\n",
       "           [1.6501e-02, 5.9476e-04, 4.0979e-05,  ..., 8.3757e-04,\n",
       "            6.2097e-04, 1.3196e-01],\n",
       "           [1.3667e-01, 1.6556e-03, 1.5563e-03,  ..., 9.2265e-05,\n",
       "            4.4803e-05, 1.3715e-02],\n",
       "           ...,\n",
       "           [1.4260e-01, 6.1275e-03, 1.3321e-03,  ..., 2.4794e-04,\n",
       "            1.2108e-04, 2.2402e-02],\n",
       "           [2.0537e-02, 1.7628e-03, 2.9436e-04,  ..., 9.4620e-04,\n",
       "            8.9081e-04, 1.0565e-02],\n",
       "           [5.2642e-03, 1.3122e-03, 5.5900e-04,  ..., 5.0143e-04,\n",
       "            2.5920e-04, 3.0597e-03]],\n",
       " \n",
       "          [[3.4182e-02, 2.3267e-04, 2.6068e-04,  ..., 4.4422e-04,\n",
       "            1.0055e-04, 2.0343e-02],\n",
       "           [3.9598e-02, 5.4865e-04, 5.2417e-04,  ..., 5.6376e-05,\n",
       "            5.8552e-05, 1.7304e-02],\n",
       "           [6.5493e-06, 9.1755e-01, 3.2171e-05,  ..., 2.4589e-09,\n",
       "            5.3024e-10, 5.9300e-06],\n",
       "           ...,\n",
       "           [1.8832e-03, 1.6296e-04, 2.9215e-05,  ..., 1.9981e-06,\n",
       "            2.3943e-06, 1.8098e-03],\n",
       "           [4.6251e-01, 9.2370e-05, 5.7185e-05,  ..., 8.8647e-06,\n",
       "            4.3526e-06, 7.5892e-02],\n",
       "           [1.5064e-02, 6.2456e-05, 1.7307e-05,  ..., 4.1388e-06,\n",
       "            8.3228e-04, 2.4235e-02]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[1.4890e-05, 3.5806e-04, 3.3310e-03,  ..., 2.1576e-04,\n",
       "            2.3602e-04, 8.2335e-06],\n",
       "           [1.2257e-05, 3.5459e-04, 2.9710e-03,  ..., 2.1659e-04,\n",
       "            2.1129e-04, 6.6404e-06],\n",
       "           [3.1707e-03, 8.7119e-01, 4.6280e-04,  ..., 2.0614e-09,\n",
       "            3.5094e-08, 5.3412e-03],\n",
       "           ...,\n",
       "           [1.5829e-04, 2.5980e-08, 8.7930e-09,  ..., 4.7295e-10,\n",
       "            1.6584e-09, 9.2263e-05],\n",
       "           [6.4740e-03, 4.7103e-06, 5.6211e-06,  ..., 4.1946e-08,\n",
       "            1.3331e-07, 2.1089e-03],\n",
       "           [1.1386e-02, 9.4080e-08, 4.7957e-07,  ..., 3.0807e-07,\n",
       "            1.5992e-07, 2.2282e-03]],\n",
       " \n",
       "          [[4.4115e-06, 7.2069e-06, 2.5693e-04,  ..., 5.3908e-05,\n",
       "            6.4617e-05, 9.3502e-01],\n",
       "           [4.8047e-06, 6.9430e-06, 2.4046e-04,  ..., 5.2360e-05,\n",
       "            6.5449e-05, 9.3602e-01],\n",
       "           [1.9518e-02, 1.8148e-02, 1.8630e-02,  ..., 2.5134e-05,\n",
       "            4.2724e-05, 1.5028e-03],\n",
       "           ...,\n",
       "           [2.6626e-02, 7.2042e-04, 1.6188e-04,  ..., 3.1522e-05,\n",
       "            2.3134e-05, 7.2830e-05],\n",
       "           [2.3296e-02, 1.1632e-03, 4.2494e-04,  ..., 5.1997e-05,\n",
       "            5.8220e-05, 1.5196e-04],\n",
       "           [1.0345e-01, 1.6555e-03, 7.0585e-04,  ..., 3.0982e-05,\n",
       "            3.6340e-05, 1.4903e-04]],\n",
       " \n",
       "          [[3.0835e-06, 1.2404e-05, 4.4528e-05,  ..., 2.6114e-04,\n",
       "            1.3579e-04, 2.2230e-05],\n",
       "           [2.2466e-06, 1.1382e-05, 4.1555e-05,  ..., 2.4527e-04,\n",
       "            1.2100e-04, 1.9915e-05],\n",
       "           [1.9263e-03, 1.0215e-01, 2.4948e-03,  ..., 1.0013e-05,\n",
       "            1.1775e-05, 1.3029e-01],\n",
       "           ...,\n",
       "           [6.7141e-05, 2.5356e-06, 6.5602e-08,  ..., 4.3660e-08,\n",
       "            9.0412e-09, 1.1344e-04],\n",
       "           [1.1655e-04, 4.3877e-05, 1.6029e-07,  ..., 1.2380e-07,\n",
       "            1.3617e-08, 1.5576e-03],\n",
       "           [1.0315e-02, 1.6859e-02, 1.6395e-03,  ..., 3.1110e-05,\n",
       "            3.3106e-05, 3.6192e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.1943e-05, 3.8101e-05, 2.2508e-05,  ..., 2.8703e-03,\n",
       "            7.4160e-03, 3.0586e-05],\n",
       "           [1.2572e-05, 4.4319e-05, 2.4188e-05,  ..., 2.7808e-03,\n",
       "            7.2766e-03, 3.4564e-05],\n",
       "           [4.8142e-02, 2.8981e-02, 4.1138e-03,  ..., 2.1038e-05,\n",
       "            1.7088e-05, 1.1998e-02],\n",
       "           ...,\n",
       "           [2.6308e-02, 1.1878e-03, 6.7751e-04,  ..., 5.4457e-04,\n",
       "            4.9515e-04, 1.5527e-02],\n",
       "           [1.1413e-02, 5.5029e-03, 1.6940e-03,  ..., 2.8524e-04,\n",
       "            2.8454e-04, 8.5142e-03],\n",
       "           [4.5491e-02, 9.7057e-05, 4.9807e-04,  ..., 2.5254e-05,\n",
       "            2.0575e-05, 1.7193e-04]],\n",
       " \n",
       "          [[4.5617e-06, 1.3004e-04, 3.8521e-04,  ..., 3.2103e-06,\n",
       "            4.1557e-07, 3.0103e-05],\n",
       "           [3.9822e-06, 1.0595e-04, 3.4915e-04,  ..., 3.3740e-06,\n",
       "            4.3964e-07, 2.1221e-05],\n",
       "           [7.4349e-03, 1.0030e-01, 9.9543e-03,  ..., 6.3428e-07,\n",
       "            3.7065e-07, 2.0109e-03],\n",
       "           ...,\n",
       "           [1.4972e-04, 1.0577e-07, 6.1638e-11,  ..., 9.9815e-10,\n",
       "            1.5397e-09, 3.6877e-05],\n",
       "           [5.0349e-03, 6.1428e-05, 5.9380e-08,  ..., 2.7984e-11,\n",
       "            6.0486e-11, 2.2409e-03],\n",
       "           [4.4177e-04, 1.9397e-04, 9.3225e-06,  ..., 2.0648e-06,\n",
       "            2.2197e-06, 8.2141e-01]],\n",
       " \n",
       "          [[1.0622e-04, 6.5749e-05, 1.1070e-04,  ..., 1.3224e-04,\n",
       "            5.2141e-05, 1.5722e-05],\n",
       "           [8.7706e-05, 6.1435e-05, 9.9511e-05,  ..., 1.2898e-04,\n",
       "            4.8204e-05, 1.4791e-05],\n",
       "           [4.6331e-04, 3.9436e-01, 1.0801e-03,  ..., 1.9344e-08,\n",
       "            8.1049e-08, 6.6521e-03],\n",
       "           ...,\n",
       "           [1.8890e-04, 1.4015e-05, 4.9451e-07,  ..., 7.0121e-08,\n",
       "            6.7200e-08, 2.6134e-04],\n",
       "           [4.5108e-05, 3.8662e-03, 1.3083e-05,  ..., 6.8569e-07,\n",
       "            8.9931e-07, 1.7554e-03],\n",
       "           [3.5942e-01, 5.9702e-04, 2.5513e-03,  ..., 1.5764e-04,\n",
       "            5.0822e-06, 7.6971e-03]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[1.6672e-04, 5.2073e-05, 2.1235e-04,  ..., 2.2181e-04,\n",
       "            3.2036e-04, 6.5674e-06],\n",
       "           [1.6788e-04, 5.2303e-05, 2.1288e-04,  ..., 2.2169e-04,\n",
       "            3.2049e-04, 6.6074e-06],\n",
       "           [6.4511e-02, 2.8319e-01, 3.7744e-02,  ..., 1.6301e-07,\n",
       "            3.9496e-07, 8.5686e-03],\n",
       "           ...,\n",
       "           [2.2313e-03, 2.2164e-05, 8.7933e-07,  ..., 7.7812e-10,\n",
       "            2.6146e-10, 6.8061e-04],\n",
       "           [2.6239e-02, 6.4983e-04, 3.0493e-05,  ..., 9.0084e-06,\n",
       "            4.5223e-06, 1.7997e-02],\n",
       "           [1.5356e-01, 5.3205e-04, 1.6056e-04,  ..., 3.5437e-06,\n",
       "            4.7601e-06, 4.7388e-03]],\n",
       " \n",
       "          [[6.0048e-03, 1.7890e-03, 7.3582e-04,  ..., 4.9683e-05,\n",
       "            5.6153e-05, 2.1229e-04],\n",
       "           [6.0126e-03, 1.7848e-03, 7.3328e-04,  ..., 4.9345e-05,\n",
       "            5.5834e-05, 2.1329e-04],\n",
       "           [5.7011e-02, 5.4627e-03, 1.9738e-03,  ..., 1.0533e-05,\n",
       "            2.8285e-05, 3.1800e-02],\n",
       "           ...,\n",
       "           [8.5175e-02, 7.0885e-05, 1.3709e-05,  ..., 4.3201e-06,\n",
       "            2.9026e-06, 7.4957e-02],\n",
       "           [3.9240e-02, 4.7490e-04, 9.7078e-06,  ..., 1.2872e-05,\n",
       "            2.9988e-06, 3.9338e-02],\n",
       "           [1.2993e-02, 3.6607e-04, 1.6851e-04,  ..., 1.4554e-05,\n",
       "            1.1379e-05, 6.9716e-03]],\n",
       " \n",
       "          [[5.6406e-03, 5.2831e-03, 1.0788e-02,  ..., 3.1870e-05,\n",
       "            6.5842e-05, 3.8001e-03],\n",
       "           [5.6827e-03, 5.2938e-03, 1.0824e-02,  ..., 3.1859e-05,\n",
       "            6.5705e-05, 3.8208e-03],\n",
       "           [2.8429e-05, 4.3743e-02, 3.8363e-04,  ..., 4.0047e-10,\n",
       "            1.2113e-09, 1.9810e-03],\n",
       "           ...,\n",
       "           [1.6680e-04, 4.2795e-08, 1.2456e-08,  ..., 3.7950e-07,\n",
       "            5.7208e-08, 1.3867e-04],\n",
       "           [6.2109e-03, 1.3693e-04, 2.1776e-06,  ..., 1.2620e-05,\n",
       "            1.0472e-06, 2.5850e-03],\n",
       "           [2.2278e-02, 1.2411e-03, 1.7118e-03,  ..., 9.8095e-06,\n",
       "            2.2786e-05, 1.0554e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[4.6275e-01, 6.2565e-04, 2.9992e-03,  ..., 3.6461e-05,\n",
       "            1.5375e-04, 1.8410e-05],\n",
       "           [4.6526e-01, 6.2568e-04, 2.9925e-03,  ..., 3.6193e-05,\n",
       "            1.5298e-04, 1.8406e-05],\n",
       "           [1.1249e-01, 1.9638e-02, 3.0363e-02,  ..., 8.4738e-07,\n",
       "            3.2098e-06, 3.8515e-03],\n",
       "           ...,\n",
       "           [5.6235e-02, 7.5733e-05, 2.0822e-07,  ..., 1.9939e-09,\n",
       "            3.2672e-09, 9.1845e-04],\n",
       "           [5.1574e-02, 5.7392e-04, 4.7224e-06,  ..., 3.5015e-07,\n",
       "            1.2556e-07, 9.7723e-03],\n",
       "           [2.7630e-01, 1.6012e-02, 7.3832e-04,  ..., 1.5997e-05,\n",
       "            3.4831e-05, 2.0811e-03]],\n",
       " \n",
       "          [[9.7238e-01, 1.6523e-07, 1.0279e-07,  ..., 8.4478e-11,\n",
       "            2.2941e-11, 8.2693e-07],\n",
       "           [9.7254e-01, 1.6415e-07, 1.0181e-07,  ..., 8.3394e-11,\n",
       "            2.2646e-11, 8.2340e-07],\n",
       "           [6.2237e-01, 9.3154e-02, 4.9619e-03,  ..., 2.8721e-07,\n",
       "            2.4903e-07, 1.0770e-01],\n",
       "           ...,\n",
       "           [1.3197e-02, 6.1532e-06, 4.4048e-09,  ..., 5.5705e-05,\n",
       "            6.6806e-06, 2.7660e-03],\n",
       "           [1.8185e-01, 2.1673e-03, 7.8045e-07,  ..., 1.5771e-06,\n",
       "            6.0106e-07, 9.4260e-02],\n",
       "           [4.8382e-02, 6.2677e-03, 6.5512e-03,  ..., 6.8976e-06,\n",
       "            4.9640e-06, 3.3786e-02]],\n",
       " \n",
       "          [[5.1658e-02, 9.2401e-03, 3.5204e-03,  ..., 8.5578e-04,\n",
       "            6.7647e-04, 1.3893e-02],\n",
       "           [5.2285e-02, 9.2473e-03, 3.5147e-03,  ..., 8.5347e-04,\n",
       "            6.7557e-04, 1.4007e-02],\n",
       "           [6.4617e-01, 5.0848e-03, 1.1031e-02,  ..., 2.9329e-05,\n",
       "            1.0029e-04, 2.4442e-02],\n",
       "           ...,\n",
       "           [6.6534e-01, 3.1752e-05, 9.8395e-06,  ..., 2.9845e-06,\n",
       "            2.0659e-06, 3.7105e-03],\n",
       "           [2.2590e-01, 1.0785e-02, 7.3688e-05,  ..., 7.5719e-05,\n",
       "            7.0873e-05, 8.3041e-03],\n",
       "           [5.2626e-01, 1.7205e-03, 2.4787e-04,  ..., 3.4999e-06,\n",
       "            1.4903e-05, 2.3195e-02]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[1.0011e-02, 3.1349e-04, 6.5821e-04,  ..., 2.8014e-04,\n",
       "            2.2539e-04, 3.7039e-04],\n",
       "           [1.0016e-02, 3.1357e-04, 6.5836e-04,  ..., 2.8012e-04,\n",
       "            2.2536e-04, 3.7055e-04],\n",
       "           [1.2352e-01, 2.4094e-02, 1.3360e-02,  ..., 9.1580e-06,\n",
       "            8.0141e-06, 6.0540e-03],\n",
       "           ...,\n",
       "           [4.4610e-02, 1.4210e-02, 1.3207e-03,  ..., 2.0612e-05,\n",
       "            6.4968e-06, 2.0691e-02],\n",
       "           [2.6734e-01, 8.9255e-03, 1.6117e-04,  ..., 4.2796e-05,\n",
       "            3.8372e-05, 2.8947e-01],\n",
       "           [5.3429e-01, 1.3651e-03, 1.6541e-04,  ..., 1.4116e-07,\n",
       "            4.9546e-07, 2.3186e-01]],\n",
       " \n",
       "          [[2.7952e-07, 6.8888e-05, 4.1715e-04,  ..., 1.9891e-04,\n",
       "            3.3893e-04, 2.6784e-06],\n",
       "           [2.7972e-07, 6.8917e-05, 4.1729e-04,  ..., 1.9891e-04,\n",
       "            3.3896e-04, 2.6797e-06],\n",
       "           [2.5207e-01, 7.4485e-03, 2.8296e-03,  ..., 4.4683e-06,\n",
       "            7.2124e-06, 4.7296e-03],\n",
       "           ...,\n",
       "           [9.3104e-02, 7.9696e-06, 5.2322e-07,  ..., 1.6615e-05,\n",
       "            4.6944e-05, 2.9516e-03],\n",
       "           [3.6090e-01, 7.1229e-05, 2.5925e-06,  ..., 1.0679e-05,\n",
       "            2.0777e-06, 1.4801e-03],\n",
       "           [3.6826e-01, 4.5442e-04, 3.7153e-04,  ..., 5.3202e-06,\n",
       "            6.2610e-07, 1.2886e-03]],\n",
       " \n",
       "          [[2.9663e-11, 4.4110e-06, 1.2924e-04,  ..., 2.0000e-03,\n",
       "            4.2906e-03, 4.6990e-07],\n",
       "           [2.9684e-11, 4.4102e-06, 1.2922e-04,  ..., 1.9992e-03,\n",
       "            4.2890e-03, 4.6981e-07],\n",
       "           [4.7554e-01, 7.9149e-03, 5.5149e-03,  ..., 6.1973e-05,\n",
       "            1.0580e-04, 6.5172e-02],\n",
       "           ...,\n",
       "           [8.6859e-01, 5.5415e-04, 5.7213e-06,  ..., 2.4717e-05,\n",
       "            1.0327e-05, 1.0067e-03],\n",
       "           [2.9184e-01, 3.1027e-03, 8.4580e-06,  ..., 1.0842e-04,\n",
       "            5.1869e-05, 1.0399e-03],\n",
       "           [8.3260e-01, 9.6561e-04, 6.1388e-05,  ..., 1.8877e-07,\n",
       "            8.0616e-08, 1.4336e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[3.5456e-01, 1.2656e-07, 1.0208e-07,  ..., 6.4225e-11,\n",
       "            1.0541e-09, 6.4444e-01],\n",
       "           [3.5459e-01, 1.2647e-07, 1.0201e-07,  ..., 6.4144e-11,\n",
       "            1.0528e-09, 6.4440e-01],\n",
       "           [4.7719e-02, 9.3333e-03, 8.8088e-03,  ..., 9.9333e-05,\n",
       "            7.8817e-05, 1.6892e-02],\n",
       "           ...,\n",
       "           [9.9202e-02, 1.7250e-03, 4.3757e-05,  ..., 5.8161e-06,\n",
       "            6.8110e-06, 2.3534e-02],\n",
       "           [3.5615e-02, 5.9536e-03, 4.1823e-04,  ..., 8.1386e-05,\n",
       "            2.7449e-05, 2.0519e-02],\n",
       "           [6.2007e-02, 1.3567e-04, 1.4758e-05,  ..., 1.1011e-06,\n",
       "            3.8397e-07, 1.4876e-02]],\n",
       " \n",
       "          [[2.9558e-06, 2.3984e-04, 4.5220e-04,  ..., 7.4444e-04,\n",
       "            7.7556e-04, 5.5720e-05],\n",
       "           [2.9569e-06, 2.3989e-04, 4.5231e-04,  ..., 7.4441e-04,\n",
       "            7.7555e-04, 5.5731e-05],\n",
       "           [5.0920e-02, 5.2501e-03, 1.8334e-03,  ..., 7.3085e-06,\n",
       "            2.6027e-05, 3.8478e-03],\n",
       "           ...,\n",
       "           [3.5593e-02, 6.6816e-04, 1.8032e-05,  ..., 8.5044e-05,\n",
       "            4.2223e-05, 9.4254e-03],\n",
       "           [2.1735e-02, 1.3555e-03, 3.3478e-05,  ..., 4.6195e-04,\n",
       "            4.3160e-04, 2.5067e-02],\n",
       "           [9.1894e-02, 2.4893e-03, 1.6134e-03,  ..., 1.0829e-05,\n",
       "            1.3556e-05, 1.2258e-03]],\n",
       " \n",
       "          [[2.7612e-12, 2.0628e-05, 4.9078e-04,  ..., 1.1852e-04,\n",
       "            5.7119e-04, 2.0595e-10],\n",
       "           [2.7641e-12, 2.0637e-05, 4.9076e-04,  ..., 1.1854e-04,\n",
       "            5.7132e-04, 2.0616e-10],\n",
       "           [1.7348e-01, 5.9812e-01, 3.0891e-05,  ..., 2.1440e-09,\n",
       "            6.0878e-08, 1.2587e-01],\n",
       "           ...,\n",
       "           [1.9860e-01, 5.4475e-05, 8.0881e-07,  ..., 8.5020e-08,\n",
       "            9.1762e-07, 2.1931e-02],\n",
       "           [5.4422e-01, 2.2882e-03, 1.8889e-06,  ..., 4.8687e-07,\n",
       "            6.8743e-07, 4.9476e-02],\n",
       "           [4.5670e-01, 1.3083e-04, 1.2661e-05,  ..., 9.2706e-07,\n",
       "            2.0347e-07, 8.5883e-02]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.0004e-11, 1.0820e-05, 4.4712e-05,  ..., 1.0925e-04,\n",
       "            2.4724e-04, 5.2024e-06],\n",
       "           [2.0005e-11, 1.0820e-05, 4.4713e-05,  ..., 1.0925e-04,\n",
       "            2.4724e-04, 5.2027e-06],\n",
       "           [1.1033e-01, 2.1013e-03, 6.8607e-04,  ..., 2.3264e-06,\n",
       "            5.2159e-06, 5.1299e-05],\n",
       "           ...,\n",
       "           [9.5943e-02, 1.4997e-04, 1.0263e-05,  ..., 2.7770e-06,\n",
       "            1.2497e-06, 5.0541e-03],\n",
       "           [4.3860e-02, 5.1876e-04, 7.4401e-05,  ..., 9.5593e-06,\n",
       "            2.0634e-06, 2.1956e-04],\n",
       "           [4.7121e-02, 4.0204e-04, 1.5700e-05,  ..., 5.9845e-07,\n",
       "            6.6724e-07, 1.5569e-03]],\n",
       " \n",
       "          [[4.3576e-12, 1.2937e-05, 2.6710e-04,  ..., 1.7418e-04,\n",
       "            1.4399e-04, 8.0793e-10],\n",
       "           [4.3581e-12, 1.2937e-05, 2.6711e-04,  ..., 1.7418e-04,\n",
       "            1.4399e-04, 8.0798e-10],\n",
       "           [9.7605e-02, 5.5034e-02, 4.9595e-02,  ..., 1.0891e-07,\n",
       "            8.2449e-08, 2.5901e-03],\n",
       "           ...,\n",
       "           [9.8304e-02, 8.2262e-06, 1.7540e-07,  ..., 6.5536e-08,\n",
       "            2.0133e-08, 2.2563e-03],\n",
       "           [4.2885e-02, 6.3621e-04, 8.8751e-06,  ..., 9.0046e-06,\n",
       "            1.4770e-05, 1.5401e-02],\n",
       "           [2.9003e-01, 4.3154e-05, 1.0184e-05,  ..., 1.8983e-08,\n",
       "            5.0617e-09, 9.6305e-04]],\n",
       " \n",
       "          [[1.5302e-14, 1.0643e-07, 8.9105e-07,  ..., 6.2739e-06,\n",
       "            1.6950e-06, 1.1714e-09],\n",
       "           [1.5303e-14, 1.0643e-07, 8.9103e-07,  ..., 6.2737e-06,\n",
       "            1.6949e-06, 1.1714e-09],\n",
       "           [3.8297e-02, 2.7421e-02, 4.0224e-03,  ..., 1.6882e-06,\n",
       "            5.3000e-06, 1.8767e-03],\n",
       "           ...,\n",
       "           [3.5725e-01, 3.3482e-05, 7.6997e-07,  ..., 4.9994e-07,\n",
       "            1.2632e-06, 1.8749e-03],\n",
       "           [1.0310e-01, 4.8744e-04, 2.2890e-05,  ..., 1.8871e-05,\n",
       "            3.2229e-05, 7.3230e-03],\n",
       "           [4.9448e-01, 4.0563e-04, 2.6752e-06,  ..., 5.7962e-07,\n",
       "            6.7958e-06, 4.0629e-04]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.1282e-09, 2.4021e-05, 6.3583e-04,  ..., 1.9397e-04,\n",
       "            6.4490e-04, 9.0743e-08],\n",
       "           [1.1282e-09, 2.4022e-05, 6.3584e-04,  ..., 1.9397e-04,\n",
       "            6.4491e-04, 9.0746e-08],\n",
       "           [6.2010e-02, 5.0899e-03, 4.6326e-03,  ..., 2.2919e-05,\n",
       "            2.8318e-05, 1.1705e-02],\n",
       "           ...,\n",
       "           [1.0444e-02, 2.1230e-03, 1.8377e-05,  ..., 1.3196e-04,\n",
       "            6.7826e-05, 5.1496e-03],\n",
       "           [1.3106e-02, 2.5781e-03, 5.9726e-05,  ..., 3.0166e-04,\n",
       "            2.7830e-04, 1.0777e-02],\n",
       "           [3.5417e-02, 5.7898e-04, 1.3980e-05,  ..., 3.4469e-07,\n",
       "            1.5986e-07, 4.2396e-04]],\n",
       " \n",
       "          [[6.3323e-10, 1.2530e-05, 2.7454e-04,  ..., 1.4184e-04,\n",
       "            1.4141e-04, 1.7102e-07],\n",
       "           [6.3326e-10, 1.2530e-05, 2.7454e-04,  ..., 1.4184e-04,\n",
       "            1.4141e-04, 1.7103e-07],\n",
       "           [1.3211e-01, 5.1033e-03, 3.9921e-03,  ..., 2.3840e-06,\n",
       "            3.2913e-06, 2.1555e-03],\n",
       "           ...,\n",
       "           [6.7162e-02, 1.8451e-05, 4.3898e-07,  ..., 3.8384e-06,\n",
       "            1.8355e-06, 9.6051e-04],\n",
       "           [1.9222e-01, 8.3125e-05, 3.5536e-06,  ..., 6.2959e-06,\n",
       "            2.4906e-06, 1.2070e-03],\n",
       "           [4.5258e-03, 6.5837e-04, 1.5091e-05,  ..., 2.0644e-06,\n",
       "            5.4772e-06, 4.8124e-04]],\n",
       " \n",
       "          [[3.3148e-12, 2.5285e-07, 9.7570e-05,  ..., 1.6556e-04,\n",
       "            1.6572e-04, 5.4883e-09],\n",
       "           [3.3152e-12, 2.5286e-07, 9.7574e-05,  ..., 1.6556e-04,\n",
       "            1.6572e-04, 5.4884e-09],\n",
       "           [8.8188e-03, 9.6197e-02, 7.0745e-02,  ..., 7.8963e-07,\n",
       "            1.7780e-06, 1.0349e-02],\n",
       "           ...,\n",
       "           [1.8676e-01, 1.5081e-05, 2.9443e-08,  ..., 3.1318e-07,\n",
       "            2.9347e-07, 7.8476e-04],\n",
       "           [2.5318e-01, 1.8356e-04, 2.7822e-06,  ..., 6.5985e-07,\n",
       "            4.7161e-07, 1.8596e-03],\n",
       "           [4.2982e-02, 2.1474e-04, 8.4865e-06,  ..., 1.5975e-06,\n",
       "            2.3442e-06, 5.2280e-03]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[1.0524e-08, 1.0475e-06, 1.0101e-04,  ..., 1.1038e-04,\n",
       "            6.0925e-04, 6.2322e-01],\n",
       "           [1.0524e-08, 1.0475e-06, 1.0101e-04,  ..., 1.1038e-04,\n",
       "            6.0925e-04, 6.2321e-01],\n",
       "           [2.7616e-02, 9.1683e-03, 2.8237e-03,  ..., 6.5916e-05,\n",
       "            3.5627e-05, 1.8204e-03],\n",
       "           ...,\n",
       "           [1.9481e-03, 2.9789e-05, 5.2240e-07,  ..., 7.7109e-06,\n",
       "            2.4182e-05, 2.1501e-05],\n",
       "           [6.3885e-02, 7.7698e-03, 4.8052e-04,  ..., 3.5022e-05,\n",
       "            4.4574e-05, 2.2987e-03],\n",
       "           [8.9631e-02, 1.3444e-03, 3.4136e-04,  ..., 4.9747e-06,\n",
       "            5.9777e-06, 1.6273e-04]],\n",
       " \n",
       "          [[9.0013e-07, 1.3834e-04, 3.6524e-03,  ..., 9.0484e-04,\n",
       "            8.3349e-04, 1.2049e-05],\n",
       "           [9.0012e-07, 1.3834e-04, 3.6524e-03,  ..., 9.0484e-04,\n",
       "            8.3349e-04, 1.2049e-05],\n",
       "           [6.0160e-03, 4.0724e-02, 2.2124e-02,  ..., 1.0740e-07,\n",
       "            4.0483e-07, 1.0045e-03],\n",
       "           ...,\n",
       "           [3.3758e-02, 2.3662e-05, 5.2867e-07,  ..., 1.5522e-10,\n",
       "            4.6706e-10, 8.7030e-04],\n",
       "           [6.1113e-02, 2.8653e-04, 4.8582e-06,  ..., 1.0658e-05,\n",
       "            2.4250e-05, 7.1850e-03],\n",
       "           [3.2652e-03, 1.6581e-04, 7.1653e-06,  ..., 2.3360e-07,\n",
       "            7.0780e-07, 1.8629e-04]],\n",
       " \n",
       "          [[2.3493e-11, 8.4890e-05, 9.2164e-05,  ..., 1.7030e-04,\n",
       "            3.9444e-03, 1.6779e-07],\n",
       "           [2.3493e-11, 8.4890e-05, 9.2165e-05,  ..., 1.7030e-04,\n",
       "            3.9444e-03, 1.6779e-07],\n",
       "           [4.7269e-02, 1.0847e-02, 8.7062e-04,  ..., 7.2244e-06,\n",
       "            1.8323e-05, 2.0507e-02],\n",
       "           ...,\n",
       "           [3.7315e-02, 6.1656e-05, 3.3605e-06,  ..., 1.5310e-07,\n",
       "            3.1420e-08, 1.7918e-03],\n",
       "           [3.1199e-02, 1.1027e-03, 3.1314e-05,  ..., 5.6032e-06,\n",
       "            3.4467e-06, 4.1189e-02],\n",
       "           [4.2196e-02, 2.2452e-04, 8.7104e-06,  ..., 1.5486e-07,\n",
       "            1.7213e-07, 3.2557e-04]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[3.0839e-07, 2.3652e-04, 2.3949e-03,  ..., 4.4799e-04,\n",
       "            2.7554e-04, 1.7097e-05],\n",
       "           [3.0839e-07, 2.3652e-04, 2.3949e-03,  ..., 4.4799e-04,\n",
       "            2.7554e-04, 1.7097e-05],\n",
       "           [1.6714e-03, 1.4789e-02, 5.7534e-02,  ..., 4.0632e-07,\n",
       "            9.6513e-07, 1.6174e-04],\n",
       "           ...,\n",
       "           [2.1282e-02, 3.7708e-04, 2.6731e-06,  ..., 1.0676e-07,\n",
       "            2.3280e-07, 9.0794e-05],\n",
       "           [1.6312e-01, 6.9094e-03, 1.4982e-04,  ..., 2.7253e-05,\n",
       "            5.2237e-05, 7.3964e-03],\n",
       "           [1.1474e-02, 6.7979e-04, 2.7292e-05,  ..., 5.3236e-07,\n",
       "            1.5398e-06, 1.0090e-03]],\n",
       " \n",
       "          [[9.6819e-16, 6.5160e-06, 9.8887e-05,  ..., 1.1763e-03,\n",
       "            1.6719e-03, 1.1128e-10],\n",
       "           [9.6819e-16, 6.5160e-06, 9.8886e-05,  ..., 1.1763e-03,\n",
       "            1.6719e-03, 1.1129e-10],\n",
       "           [3.0471e-02, 2.6321e-03, 2.2638e-03,  ..., 6.3365e-05,\n",
       "            2.7562e-05, 8.3564e-03],\n",
       "           ...,\n",
       "           [6.1198e-02, 4.3768e-04, 6.2694e-05,  ..., 8.2940e-06,\n",
       "            1.1632e-04, 5.1601e-03],\n",
       "           [7.0861e-03, 4.6714e-03, 5.9786e-04,  ..., 4.7440e-05,\n",
       "            6.2041e-05, 1.7186e-03],\n",
       "           [2.0652e-02, 3.1041e-04, 1.2449e-04,  ..., 7.8355e-06,\n",
       "            4.6230e-06, 5.4174e-04]],\n",
       " \n",
       "          [[3.6636e-11, 4.8585e-07, 5.3713e-07,  ..., 1.9573e-04,\n",
       "            1.3481e-05, 2.2004e-11],\n",
       "           [3.6635e-11, 4.8585e-07, 5.3713e-07,  ..., 1.9572e-04,\n",
       "            1.3481e-05, 2.2004e-11],\n",
       "           [1.5158e-01, 2.0654e-05, 3.5874e-04,  ..., 4.8007e-07,\n",
       "            4.9242e-05, 6.2517e-06],\n",
       "           ...,\n",
       "           [8.7731e-02, 2.6184e-04, 4.3702e-05,  ..., 5.2741e-07,\n",
       "            5.2013e-07, 7.3333e-05],\n",
       "           [3.5524e-02, 1.4256e-04, 5.0863e-05,  ..., 8.0534e-06,\n",
       "            5.6931e-06, 4.9836e-04],\n",
       "           [2.6494e-01, 1.9832e-06, 3.2441e-06,  ..., 1.3934e-06,\n",
       "            1.1768e-05, 6.9322e-05]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.7015e-05, 4.1797e-04, 1.5999e-04,  ..., 7.3730e-04,\n",
       "            1.9314e-03, 1.4679e-02],\n",
       "           [2.7015e-05, 4.1797e-04, 1.5999e-04,  ..., 7.3730e-04,\n",
       "            1.9314e-03, 1.4679e-02],\n",
       "           [4.7492e-01, 3.6153e-03, 4.8272e-04,  ..., 1.0519e-04,\n",
       "            2.8949e-05, 1.0310e-01],\n",
       "           ...,\n",
       "           [1.0767e-01, 1.2146e-03, 5.3442e-04,  ..., 7.9535e-05,\n",
       "            8.2902e-05, 1.2031e-01],\n",
       "           [1.5420e-03, 1.8397e-03, 2.4782e-04,  ..., 1.0727e-04,\n",
       "            8.1742e-05, 1.0728e-01],\n",
       "           [6.6976e-01, 9.6861e-04, 6.6934e-04,  ..., 3.7661e-05,\n",
       "            1.8337e-05, 9.0549e-02]],\n",
       " \n",
       "          [[9.0804e-01, 2.3569e-09, 1.8016e-07,  ..., 1.4246e-09,\n",
       "            3.5008e-11, 2.9210e-03],\n",
       "           [9.0804e-01, 2.3569e-09, 1.8016e-07,  ..., 1.4245e-09,\n",
       "            3.5008e-11, 2.9210e-03],\n",
       "           [9.0066e-01, 1.5891e-03, 4.6853e-04,  ..., 2.1378e-05,\n",
       "            8.0835e-06, 1.0804e-02],\n",
       "           ...,\n",
       "           [4.6180e-01, 2.7571e-04, 4.5407e-04,  ..., 2.4193e-04,\n",
       "            1.5950e-04, 1.8250e-02],\n",
       "           [8.1694e-01, 4.3476e-04, 3.6002e-04,  ..., 9.0920e-05,\n",
       "            4.8525e-05, 2.1082e-02],\n",
       "           [4.3025e-01, 1.0496e-03, 9.4262e-05,  ..., 1.7363e-06,\n",
       "            3.5977e-07, 2.2515e-03]],\n",
       " \n",
       "          [[2.8626e-01, 3.7260e-05, 3.0969e-05,  ..., 2.5582e-05,\n",
       "            1.5584e-05, 2.3938e-03],\n",
       "           [2.8626e-01, 3.7260e-05, 3.0969e-05,  ..., 2.5582e-05,\n",
       "            1.5584e-05, 2.3938e-03],\n",
       "           [2.4927e-02, 5.1167e-03, 6.6791e-04,  ..., 6.6970e-06,\n",
       "            4.6754e-06, 2.0908e-03],\n",
       "           ...,\n",
       "           [3.9132e-02, 3.4164e-04, 6.9678e-05,  ..., 8.3247e-05,\n",
       "            1.7397e-05, 4.9796e-03],\n",
       "           [1.7902e-02, 2.8593e-03, 4.7787e-04,  ..., 1.0822e-05,\n",
       "            3.2774e-05, 1.1972e-02],\n",
       "           [5.3402e-03, 6.1965e-04, 2.8384e-04,  ..., 5.1139e-06,\n",
       "            3.1231e-06, 3.2930e-04]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.0201e-01, 2.1467e-05, 5.5469e-06,  ..., 3.5049e-04,\n",
       "            1.8123e-03, 5.2890e-04],\n",
       "           [1.0201e-01, 2.1467e-05, 5.5469e-06,  ..., 3.5049e-04,\n",
       "            1.8123e-03, 5.2889e-04],\n",
       "           [4.7225e-02, 2.2159e-03, 3.8276e-04,  ..., 1.7944e-05,\n",
       "            2.3856e-05, 2.0903e-03],\n",
       "           ...,\n",
       "           [1.3466e-02, 7.2203e-06, 4.7272e-07,  ..., 5.1231e-07,\n",
       "            7.6428e-07, 1.7221e-03],\n",
       "           [3.6066e-03, 3.3720e-04, 1.4337e-05,  ..., 7.9919e-05,\n",
       "            6.1491e-05, 6.4866e-03],\n",
       "           [4.5650e-02, 8.6230e-06, 1.2062e-06,  ..., 1.3127e-05,\n",
       "            1.1966e-05, 4.2030e-04]],\n",
       " \n",
       "          [[1.1001e-03, 2.7660e-03, 5.1740e-04,  ..., 7.2858e-04,\n",
       "            4.0821e-04, 4.0282e-02],\n",
       "           [1.1001e-03, 2.7660e-03, 5.1740e-04,  ..., 7.2858e-04,\n",
       "            4.0821e-04, 4.0282e-02],\n",
       "           [1.0849e-01, 1.9961e-03, 1.5939e-03,  ..., 5.0165e-06,\n",
       "            7.4926e-06, 4.4174e-03],\n",
       "           ...,\n",
       "           [3.6339e-03, 4.3221e-03, 1.4787e-04,  ..., 3.1816e-05,\n",
       "            1.5376e-05, 1.2495e-02],\n",
       "           [3.3540e-03, 1.8903e-02, 5.3337e-04,  ..., 1.0327e-04,\n",
       "            7.9469e-05, 4.1672e-02],\n",
       "           [3.0972e-01, 3.9899e-04, 2.4296e-04,  ..., 1.2186e-05,\n",
       "            6.9265e-06, 6.8261e-03]],\n",
       " \n",
       "          [[5.9672e-01, 2.6297e-04, 4.5102e-05,  ..., 4.6554e-05,\n",
       "            7.9378e-05, 1.0121e-07],\n",
       "           [5.9673e-01, 2.6297e-04, 4.5102e-05,  ..., 4.6554e-05,\n",
       "            7.9378e-05, 1.0121e-07],\n",
       "           [1.8268e-02, 6.6965e-03, 2.8195e-02,  ..., 8.1954e-07,\n",
       "            2.2560e-06, 2.5195e-03],\n",
       "           ...,\n",
       "           [7.9188e-02, 1.3176e-04, 4.6229e-06,  ..., 1.0937e-07,\n",
       "            7.2335e-08, 9.2992e-03],\n",
       "           [3.3266e-02, 3.4769e-04, 2.2172e-05,  ..., 1.7917e-05,\n",
       "            1.7302e-05, 7.9837e-03],\n",
       "           [8.3996e-02, 2.4081e-03, 1.2562e-03,  ..., 1.0353e-04,\n",
       "            1.2958e-04, 1.9369e-03]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[8.2752e-01, 3.2082e-06, 3.2797e-05,  ..., 2.6652e-07,\n",
       "            2.9966e-07, 2.5078e-02],\n",
       "           [8.2752e-01, 3.2082e-06, 3.2797e-05,  ..., 2.6652e-07,\n",
       "            2.9966e-07, 2.5078e-02],\n",
       "           [8.4526e-03, 1.8645e-02, 1.6411e-02,  ..., 8.5607e-05,\n",
       "            1.9875e-04, 3.2803e-05],\n",
       "           ...,\n",
       "           [2.5940e-02, 7.7348e-04, 9.7873e-06,  ..., 5.5507e-06,\n",
       "            1.5414e-06, 4.9104e-02],\n",
       "           [1.0534e-02, 2.2258e-03, 3.3607e-05,  ..., 4.7504e-05,\n",
       "            2.4723e-05, 2.1369e-02],\n",
       "           [3.0268e-02, 8.5894e-04, 6.6491e-05,  ..., 1.3190e-05,\n",
       "            9.8268e-06, 5.3870e-04]],\n",
       " \n",
       "          [[1.5007e-26, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 9.5421e-31],\n",
       "           [1.5007e-26, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 9.5421e-31],\n",
       "           [2.7227e-01, 2.9185e-02, 1.8841e-03,  ..., 3.9565e-06,\n",
       "            3.8893e-06, 4.5087e-03],\n",
       "           ...,\n",
       "           [3.7472e-02, 7.5461e-04, 4.9375e-05,  ..., 3.6574e-06,\n",
       "            1.9459e-06, 2.2879e-02],\n",
       "           [5.7400e-02, 1.4797e-03, 2.7661e-05,  ..., 2.2020e-05,\n",
       "            1.2316e-05, 1.3880e-02],\n",
       "           [3.5330e-01, 1.2205e-03, 7.2235e-05,  ..., 6.0629e-05,\n",
       "            2.6076e-05, 5.0597e-03]],\n",
       " \n",
       "          [[2.5709e-01, 4.7526e-08, 6.0574e-09,  ..., 1.9013e-08,\n",
       "            8.6710e-07, 1.2903e-07],\n",
       "           [2.5709e-01, 4.7526e-08, 6.0574e-09,  ..., 1.9013e-08,\n",
       "            8.6710e-07, 1.2903e-07],\n",
       "           [2.0030e-02, 5.4351e-02, 1.9713e-03,  ..., 5.9052e-07,\n",
       "            1.4184e-06, 1.2675e-03],\n",
       "           ...,\n",
       "           [7.1884e-04, 5.1583e-05, 7.5355e-07,  ..., 1.0201e-05,\n",
       "            4.5754e-06, 8.5784e-03],\n",
       "           [1.6471e-02, 5.7519e-05, 1.3139e-06,  ..., 5.4002e-05,\n",
       "            7.3833e-06, 7.2050e-03],\n",
       "           [4.0903e-01, 8.1060e-05, 1.5553e-05,  ..., 6.1413e-05,\n",
       "            1.1413e-05, 1.3818e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[3.5932e-01, 4.1956e-05, 5.7970e-06,  ..., 1.7594e-08,\n",
       "            2.3995e-08, 2.1194e-04],\n",
       "           [3.5932e-01, 4.1956e-05, 5.7970e-06,  ..., 1.7594e-08,\n",
       "            2.3995e-08, 2.1194e-04],\n",
       "           [1.2147e-01, 6.8881e-03, 6.6266e-03,  ..., 3.9203e-06,\n",
       "            9.7092e-06, 2.5915e-03],\n",
       "           ...,\n",
       "           [3.3457e-03, 1.8979e-04, 2.0677e-04,  ..., 4.2151e-08,\n",
       "            5.4729e-08, 3.0251e-03],\n",
       "           [1.0051e-04, 1.5247e-04, 1.0693e-05,  ..., 7.1294e-06,\n",
       "            8.4010e-06, 7.5019e-03],\n",
       "           [1.5827e-03, 4.0297e-04, 1.8007e-04,  ..., 8.8337e-06,\n",
       "            3.4732e-05, 2.2852e-03]],\n",
       " \n",
       "          [[2.6296e-09, 1.5712e-12, 6.0499e-13,  ..., 1.7396e-16,\n",
       "            2.3907e-17, 1.4560e-07],\n",
       "           [2.6296e-09, 1.5712e-12, 6.0499e-13,  ..., 1.7396e-16,\n",
       "            2.3907e-17, 1.4560e-07],\n",
       "           [5.7745e-02, 1.2323e-04, 6.0786e-04,  ..., 7.8461e-07,\n",
       "            4.1760e-07, 7.2287e-03],\n",
       "           ...,\n",
       "           [2.3803e-01, 7.4000e-04, 4.3456e-06,  ..., 8.3512e-05,\n",
       "            3.8088e-05, 4.8162e-02],\n",
       "           [4.4257e-01, 3.7462e-02, 1.4854e-05,  ..., 1.3489e-05,\n",
       "            9.8528e-06, 5.0321e-02],\n",
       "           [4.8264e-01, 3.2952e-03, 8.2172e-04,  ..., 8.9957e-06,\n",
       "            7.3244e-06, 4.8397e-02]],\n",
       " \n",
       "          [[5.2280e-01, 3.9493e-06, 1.9524e-07,  ..., 5.1201e-06,\n",
       "            1.4839e-06, 2.9579e-05],\n",
       "           [5.2280e-01, 3.9493e-06, 1.9524e-07,  ..., 5.1201e-06,\n",
       "            1.4840e-06, 2.9579e-05],\n",
       "           [2.0286e-02, 1.4060e-03, 3.9251e-03,  ..., 3.6928e-07,\n",
       "            8.4544e-07, 2.4992e-04],\n",
       "           ...,\n",
       "           [1.2165e-01, 3.5031e-04, 4.3356e-06,  ..., 7.7582e-08,\n",
       "            8.9143e-08, 5.6360e-03],\n",
       "           [5.1070e-02, 1.2543e-04, 1.2967e-06,  ..., 1.7332e-05,\n",
       "            1.9327e-05, 5.9963e-03],\n",
       "           [5.5881e-02, 1.7222e-03, 2.8858e-04,  ..., 8.2139e-05,\n",
       "            8.4512e-05, 9.5281e-04]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[4.1534e-01, 2.2717e-04, 2.4712e-04,  ..., 8.6604e-05,\n",
       "            4.2496e-05, 3.8721e-03],\n",
       "           [4.1534e-01, 2.2717e-04, 2.4713e-04,  ..., 8.6604e-05,\n",
       "            4.2496e-05, 3.8721e-03],\n",
       "           [1.6946e-01, 5.7256e-02, 1.3319e-02,  ..., 1.3054e-06,\n",
       "            7.3264e-06, 1.0071e-02],\n",
       "           ...,\n",
       "           [2.0428e-01, 2.6351e-05, 1.3703e-06,  ..., 4.3445e-07,\n",
       "            1.6084e-07, 3.2795e-03],\n",
       "           [1.0806e-01, 1.7377e-04, 9.9335e-06,  ..., 1.2758e-06,\n",
       "            1.2278e-05, 3.1626e-03],\n",
       "           [2.3224e-01, 2.5099e-03, 2.7197e-04,  ..., 1.2626e-04,\n",
       "            1.0394e-04, 3.4788e-03]],\n",
       " \n",
       "          [[5.6707e-02, 2.0133e-04, 1.8565e-05,  ..., 1.5888e-05,\n",
       "            3.2939e-05, 3.8198e-03],\n",
       "           [5.6707e-02, 2.0133e-04, 1.8565e-05,  ..., 1.5888e-05,\n",
       "            3.2939e-05, 3.8198e-03],\n",
       "           [7.5124e-01, 6.2263e-03, 2.7208e-03,  ..., 4.1858e-06,\n",
       "            3.3027e-06, 2.8353e-03],\n",
       "           ...,\n",
       "           [3.6046e-01, 1.7941e-03, 1.9091e-05,  ..., 6.0600e-08,\n",
       "            9.4914e-08, 2.1891e-02],\n",
       "           [2.3651e-02, 9.2436e-04, 1.4093e-05,  ..., 2.6180e-06,\n",
       "            2.7044e-06, 6.0062e-02],\n",
       "           [2.0246e-01, 1.3163e-03, 6.4074e-05,  ..., 1.1439e-05,\n",
       "            7.9223e-06, 1.3400e-02]],\n",
       " \n",
       "          [[6.9202e-03, 1.0439e-04, 5.5957e-06,  ..., 4.1319e-04,\n",
       "            1.1338e-04, 2.0921e-03],\n",
       "           [6.9202e-03, 1.0439e-04, 5.5957e-06,  ..., 4.1319e-04,\n",
       "            1.1338e-04, 2.0921e-03],\n",
       "           [5.3186e-03, 6.2865e-01, 4.5570e-03,  ..., 1.5865e-10,\n",
       "            6.2788e-11, 8.4012e-03],\n",
       "           ...,\n",
       "           [4.9687e-04, 1.0892e-07, 1.4982e-07,  ..., 2.5083e-09,\n",
       "            5.5901e-11, 5.5681e-03],\n",
       "           [1.4806e-02, 3.8963e-04, 6.1628e-06,  ..., 1.3168e-06,\n",
       "            2.0294e-06, 1.6385e-02],\n",
       "           [1.1327e-03, 1.4340e-06, 6.4022e-06,  ..., 8.7682e-09,\n",
       "            1.1996e-08, 7.3756e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[9.0863e-02, 5.9337e-04, 9.3140e-04,  ..., 4.5525e-05,\n",
       "            3.0353e-05, 7.8901e-03],\n",
       "           [9.0863e-02, 5.9337e-04, 9.3140e-04,  ..., 4.5525e-05,\n",
       "            3.0353e-05, 7.8901e-03],\n",
       "           [1.7949e-01, 1.3074e-05, 1.1865e-04,  ..., 6.0729e-05,\n",
       "            5.4188e-05, 9.8743e-03],\n",
       "           ...,\n",
       "           [7.7305e-01, 1.2271e-04, 9.1566e-05,  ..., 3.5434e-06,\n",
       "            7.1439e-07, 1.0422e-02],\n",
       "           [9.1768e-02, 9.0885e-03, 3.7356e-04,  ..., 8.7727e-06,\n",
       "            3.4437e-06, 1.0263e-02],\n",
       "           [1.5189e-01, 7.9341e-05, 4.9643e-05,  ..., 1.4657e-04,\n",
       "            7.0415e-05, 2.4520e-02]],\n",
       " \n",
       "          [[9.0587e-02, 2.8247e-05, 6.7583e-05,  ..., 3.5501e-08,\n",
       "            6.1235e-07, 1.8082e-04],\n",
       "           [9.0587e-02, 2.8247e-05, 6.7583e-05,  ..., 3.5501e-08,\n",
       "            6.1235e-07, 1.8082e-04],\n",
       "           [7.5760e-02, 4.5647e-04, 4.3757e-04,  ..., 3.8081e-07,\n",
       "            1.1112e-06, 1.6092e-04],\n",
       "           ...,\n",
       "           [3.6901e-01, 1.1422e-04, 4.6958e-06,  ..., 4.9585e-08,\n",
       "            6.7408e-08, 7.7944e-02],\n",
       "           [2.2313e-02, 1.2067e-03, 7.6115e-05,  ..., 1.7525e-05,\n",
       "            1.5030e-05, 2.2210e-03],\n",
       "           [1.8567e-01, 3.1759e-04, 1.5250e-04,  ..., 9.9225e-06,\n",
       "            7.8839e-06, 3.7230e-03]],\n",
       " \n",
       "          [[2.3484e-01, 5.3266e-05, 1.5438e-05,  ..., 3.2743e-06,\n",
       "            1.2164e-05, 2.5034e-04],\n",
       "           [2.3484e-01, 5.3266e-05, 1.5438e-05,  ..., 3.2743e-06,\n",
       "            1.2164e-05, 2.5034e-04],\n",
       "           [6.5239e-02, 1.0176e-02, 6.1183e-03,  ..., 1.0624e-06,\n",
       "            7.9893e-06, 3.6622e-04],\n",
       "           ...,\n",
       "           [1.3006e-01, 1.1031e-04, 1.7673e-05,  ..., 1.8131e-08,\n",
       "            4.5683e-08, 1.1786e-03],\n",
       "           [7.7370e-02, 1.4159e-04, 1.3579e-05,  ..., 8.9927e-06,\n",
       "            1.3856e-05, 1.0721e-03],\n",
       "           [2.2643e-01, 6.5185e-04, 1.0760e-04,  ..., 6.2673e-05,\n",
       "            2.1000e-04, 1.2057e-03]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.5709e-01, 6.6960e-03, 2.9654e-04,  ..., 2.8013e-05,\n",
       "            5.3406e-05, 9.5374e-03],\n",
       "           [2.5709e-01, 6.6960e-03, 2.9654e-04,  ..., 2.8013e-05,\n",
       "            5.3406e-05, 9.5374e-03],\n",
       "           [3.1911e-01, 2.2436e-03, 1.1270e-04,  ..., 7.5999e-07,\n",
       "            2.9390e-06, 4.0306e-04],\n",
       "           ...,\n",
       "           [9.1064e-02, 4.3208e-04, 4.9485e-05,  ..., 3.1045e-08,\n",
       "            1.2158e-07, 5.3943e-04],\n",
       "           [3.4668e-02, 9.5426e-03, 2.6793e-04,  ..., 4.1226e-05,\n",
       "            4.9608e-05, 1.0245e-03],\n",
       "           [2.1098e-01, 5.3750e-03, 4.9458e-04,  ..., 2.6893e-05,\n",
       "            5.1320e-05, 1.0889e-03]],\n",
       " \n",
       "          [[1.7009e-01, 7.0245e-04, 3.1644e-04,  ..., 5.2615e-05,\n",
       "            5.9148e-05, 1.4892e-01],\n",
       "           [1.7009e-01, 7.0245e-04, 3.1644e-04,  ..., 5.2615e-05,\n",
       "            5.9148e-05, 1.4892e-01],\n",
       "           [2.3357e-02, 1.9147e-02, 4.9512e-02,  ..., 1.9660e-07,\n",
       "            1.4681e-06, 4.9217e-04],\n",
       "           ...,\n",
       "           [1.1899e-03, 1.1861e-04, 2.1018e-05,  ..., 3.2118e-09,\n",
       "            2.4183e-09, 2.4176e-03],\n",
       "           [4.8186e-03, 6.5782e-03, 4.0832e-04,  ..., 1.5211e-06,\n",
       "            1.7984e-06, 2.5594e-03],\n",
       "           [3.3832e-02, 3.3401e-03, 3.0387e-03,  ..., 7.4642e-06,\n",
       "            2.9705e-05, 4.2409e-03]],\n",
       " \n",
       "          [[8.0097e-04, 5.7610e-03, 6.7592e-03,  ..., 6.9640e-06,\n",
       "            5.3394e-06, 2.3423e-05],\n",
       "           [8.0097e-04, 5.7610e-03, 6.7592e-03,  ..., 6.9640e-06,\n",
       "            5.3394e-06, 2.3423e-05],\n",
       "           [2.4294e-01, 1.9382e-02, 6.6807e-02,  ..., 2.4437e-08,\n",
       "            1.0194e-07, 1.4569e-02],\n",
       "           ...,\n",
       "           [2.0603e-01, 8.1511e-06, 7.8300e-09,  ..., 9.0436e-12,\n",
       "            1.2953e-11, 2.3323e-03],\n",
       "           [4.8356e-02, 7.4461e-04, 2.7780e-06,  ..., 2.5774e-07,\n",
       "            4.3719e-07, 2.4436e-03],\n",
       "           [1.1500e-01, 7.6495e-04, 3.5719e-05,  ..., 2.7049e-06,\n",
       "            4.5771e-06, 4.2459e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[3.2484e-02, 1.1820e-01, 1.7816e-04,  ..., 2.0329e-05,\n",
       "            2.6536e-05, 1.3189e-02],\n",
       "           [3.2484e-02, 1.1820e-01, 1.7816e-04,  ..., 2.0329e-05,\n",
       "            2.6536e-05, 1.3189e-02],\n",
       "           [1.5550e-05, 2.0870e-05, 8.7342e-01,  ..., 1.9888e-11,\n",
       "            6.9151e-11, 3.8396e-04],\n",
       "           ...,\n",
       "           [2.4495e-04, 5.5268e-07, 1.5163e-11,  ..., 5.9114e-11,\n",
       "            6.1936e-11, 7.9834e-02],\n",
       "           [2.8447e-03, 1.6237e-02, 6.4418e-06,  ..., 3.7194e-07,\n",
       "            3.2520e-07, 2.5857e-02],\n",
       "           [6.7775e-05, 6.7160e-05, 1.6420e-05,  ..., 3.0898e-07,\n",
       "            1.2918e-06, 4.5043e-03]],\n",
       " \n",
       "          [[4.0624e-03, 7.7146e-03, 8.6247e-03,  ..., 6.5780e-06,\n",
       "            2.5512e-05, 3.2876e-03],\n",
       "           [4.0624e-03, 7.7146e-03, 8.6247e-03,  ..., 6.5780e-06,\n",
       "            2.5512e-05, 3.2876e-03],\n",
       "           [6.2272e-02, 1.5870e-02, 1.7127e-02,  ..., 1.0131e-07,\n",
       "            6.0207e-07, 6.5847e-03],\n",
       "           ...,\n",
       "           [7.1224e-02, 4.1059e-04, 8.9088e-05,  ..., 5.9599e-08,\n",
       "            1.7560e-08, 2.4249e-03],\n",
       "           [3.6060e-02, 7.2076e-04, 1.0921e-04,  ..., 1.7401e-05,\n",
       "            2.5120e-05, 3.4210e-03],\n",
       "           [8.7040e-02, 6.8674e-04, 7.5707e-04,  ..., 4.1657e-06,\n",
       "            1.3121e-05, 1.3545e-03]],\n",
       " \n",
       "          [[6.3707e-03, 2.0848e-04, 2.6074e-04,  ..., 1.9778e-05,\n",
       "            5.4557e-05, 1.6894e-03],\n",
       "           [6.3707e-03, 2.0848e-04, 2.6074e-04,  ..., 1.9778e-05,\n",
       "            5.4557e-05, 1.6895e-03],\n",
       "           [3.8634e-03, 2.2570e-01, 5.6411e-04,  ..., 2.2622e-08,\n",
       "            6.0782e-08, 1.4659e-03],\n",
       "           ...,\n",
       "           [1.7817e-03, 6.8531e-07, 1.7222e-07,  ..., 1.8942e-09,\n",
       "            3.0770e-09, 6.5960e-04],\n",
       "           [9.2616e-03, 1.3068e-03, 1.0479e-04,  ..., 6.5051e-06,\n",
       "            1.5462e-05, 4.0308e-03],\n",
       "           [1.7859e-02, 4.5926e-03, 1.0210e-04,  ..., 8.1346e-07,\n",
       "            1.3906e-06, 1.5300e-02]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[8.1484e-03, 5.3543e-02, 2.9345e-03,  ..., 1.4257e-05,\n",
       "            2.2037e-05, 2.4186e-03],\n",
       "           [8.1484e-03, 5.3543e-02, 2.9345e-03,  ..., 1.4257e-05,\n",
       "            2.2037e-05, 2.4186e-03],\n",
       "           [5.3095e-02, 1.6318e-03, 4.1219e-03,  ..., 5.3632e-06,\n",
       "            1.5909e-06, 1.0844e-03],\n",
       "           ...,\n",
       "           [1.1488e-02, 8.9389e-04, 1.9472e-05,  ..., 1.9993e-06,\n",
       "            7.9911e-07, 5.0680e-03],\n",
       "           [1.9620e-03, 5.7788e-03, 4.8001e-05,  ..., 5.0858e-05,\n",
       "            3.5263e-05, 7.4253e-03],\n",
       "           [1.4677e-02, 3.6486e-03, 8.0514e-04,  ..., 9.3456e-04,\n",
       "            3.7804e-04, 6.2772e-03]],\n",
       " \n",
       "          [[1.8141e-02, 2.8495e-01, 1.4647e-05,  ..., 1.3572e-08,\n",
       "            1.6014e-08, 3.6833e-03],\n",
       "           [1.8141e-02, 2.8495e-01, 1.4647e-05,  ..., 1.3572e-08,\n",
       "            1.6014e-08, 3.6833e-03],\n",
       "           [7.1299e-03, 1.9707e-04, 7.2295e-01,  ..., 3.1485e-11,\n",
       "            8.1415e-11, 5.4183e-04],\n",
       "           ...,\n",
       "           [6.5466e-03, 1.9163e-08, 2.4934e-15,  ..., 1.4361e-11,\n",
       "            1.1007e-11, 2.3832e-04],\n",
       "           [2.8182e-02, 1.3075e-04, 6.2679e-10,  ..., 3.2737e-08,\n",
       "            3.9852e-08, 4.3480e-03],\n",
       "           [1.3930e-02, 6.5506e-05, 2.4809e-05,  ..., 1.5511e-06,\n",
       "            1.4774e-06, 3.6184e-03]],\n",
       " \n",
       "          [[4.1624e-03, 2.1063e-03, 4.8374e-03,  ..., 1.6821e-05,\n",
       "            8.3879e-06, 8.8915e-03],\n",
       "           [4.1624e-03, 2.1063e-03, 4.8374e-03,  ..., 1.6821e-05,\n",
       "            8.3879e-06, 8.8915e-03],\n",
       "           [5.9902e-02, 1.6073e-04, 2.8710e-04,  ..., 3.1032e-07,\n",
       "            4.9152e-07, 2.9479e-04],\n",
       "           ...,\n",
       "           [3.3279e-01, 3.5102e-05, 7.6172e-06,  ..., 7.2693e-08,\n",
       "            3.9528e-08, 6.2553e-03],\n",
       "           [1.6779e-02, 2.2011e-04, 3.7613e-05,  ..., 3.2721e-05,\n",
       "            1.2091e-05, 1.2797e-02],\n",
       "           [5.1715e-02, 2.1979e-04, 2.0401e-04,  ..., 3.9502e-05,\n",
       "            3.0697e-05, 3.0672e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.9029e-03, 1.2075e-01, 5.3503e-04,  ..., 2.2341e-06,\n",
       "            5.3871e-06, 1.4592e-03],\n",
       "           [2.9029e-03, 1.2075e-01, 5.3503e-04,  ..., 2.2341e-06,\n",
       "            5.3871e-06, 1.4592e-03],\n",
       "           [2.0386e-02, 1.7556e-02, 4.7279e-01,  ..., 2.7515e-08,\n",
       "            3.4548e-08, 5.6981e-03],\n",
       "           ...,\n",
       "           [1.6582e-02, 9.4973e-05, 6.9541e-08,  ..., 6.4483e-07,\n",
       "            6.0599e-07, 8.6248e-03],\n",
       "           [1.7727e-02, 1.4386e-02, 2.4481e-05,  ..., 8.3554e-06,\n",
       "            2.3592e-05, 9.9032e-03],\n",
       "           [1.6880e-02, 5.9707e-04, 1.3534e-03,  ..., 1.7867e-05,\n",
       "            1.6112e-05, 3.7972e-03]],\n",
       " \n",
       "          [[7.7811e-04, 1.3523e-02, 8.2229e-03,  ..., 1.0204e-05,\n",
       "            1.5631e-05, 4.9895e-04],\n",
       "           [7.7811e-04, 1.3523e-02, 8.2229e-03,  ..., 1.0204e-05,\n",
       "            1.5631e-05, 4.9895e-04],\n",
       "           [5.2492e-02, 5.9807e-04, 8.5439e-04,  ..., 5.0694e-07,\n",
       "            2.3792e-06, 2.9624e-04],\n",
       "           ...,\n",
       "           [4.7920e-02, 1.9606e-04, 9.8745e-06,  ..., 3.1075e-08,\n",
       "            1.3199e-08, 2.0243e-03],\n",
       "           [1.2507e-01, 3.8194e-04, 3.3465e-05,  ..., 1.6422e-05,\n",
       "            5.6579e-06, 3.0416e-02],\n",
       "           [1.8039e-02, 1.2279e-03, 7.5956e-04,  ..., 1.1209e-05,\n",
       "            2.5829e-05, 3.6011e-03]],\n",
       " \n",
       "          [[3.5734e-02, 1.3263e-02, 2.9709e-03,  ..., 6.5289e-06,\n",
       "            1.7045e-05, 1.3280e-03],\n",
       "           [3.5734e-02, 1.3263e-02, 2.9709e-03,  ..., 6.5289e-06,\n",
       "            1.7045e-05, 1.3280e-03],\n",
       "           [4.6505e-02, 7.1585e-04, 2.7967e-04,  ..., 4.7227e-07,\n",
       "            3.6035e-07, 1.2401e-03],\n",
       "           ...,\n",
       "           [4.3863e-02, 1.2768e-04, 2.7396e-06,  ..., 1.1034e-07,\n",
       "            1.2679e-07, 3.1514e-04],\n",
       "           [3.6064e-02, 1.3578e-03, 4.1125e-05,  ..., 7.6654e-06,\n",
       "            2.7570e-05, 3.9056e-03],\n",
       "           [1.0866e-01, 7.0418e-04, 4.1232e-05,  ..., 1.5494e-05,\n",
       "            1.3759e-05, 8.7535e-04]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[4.9823e-03, 7.5519e-02, 1.1986e-03,  ..., 2.9077e-05,\n",
       "            2.0182e-05, 5.4988e-03],\n",
       "           [4.9823e-03, 7.5519e-02, 1.1986e-03,  ..., 2.9077e-05,\n",
       "            2.0182e-05, 5.4989e-03],\n",
       "           [9.5770e-03, 4.3307e-04, 3.0565e-03,  ..., 5.5932e-06,\n",
       "            3.8303e-06, 1.4523e-03],\n",
       "           ...,\n",
       "           [1.3209e-02, 2.3531e-04, 1.1892e-05,  ..., 5.3265e-06,\n",
       "            2.0495e-06, 1.8054e-03],\n",
       "           [1.0265e-02, 8.2140e-03, 8.9760e-05,  ..., 3.4154e-05,\n",
       "            1.9087e-05, 4.4538e-03],\n",
       "           [4.7739e-02, 3.5932e-04, 3.6354e-04,  ..., 6.6620e-05,\n",
       "            2.8043e-05, 7.6509e-03]],\n",
       " \n",
       "          [[3.9042e-02, 3.0938e-02, 9.2346e-04,  ..., 9.9656e-06,\n",
       "            6.6242e-06, 5.8888e-03],\n",
       "           [3.9042e-02, 3.0938e-02, 9.2346e-04,  ..., 9.9656e-06,\n",
       "            6.6242e-06, 5.8888e-03],\n",
       "           [4.1617e-02, 2.1481e-03, 1.2489e-02,  ..., 8.1599e-07,\n",
       "            1.0608e-06, 9.3597e-03],\n",
       "           ...,\n",
       "           [8.9594e-04, 8.1254e-04, 9.3907e-05,  ..., 1.0086e-07,\n",
       "            7.8765e-08, 2.0419e-02],\n",
       "           [1.5053e-02, 2.7473e-03, 1.9910e-04,  ..., 3.6949e-06,\n",
       "            2.8367e-06, 4.9528e-03],\n",
       "           [2.2243e-02, 2.2358e-03, 2.1593e-03,  ..., 1.5794e-05,\n",
       "            1.2267e-05, 8.4522e-03]],\n",
       " \n",
       "          [[9.8799e-02, 3.3742e-02, 6.4992e-03,  ..., 9.9443e-06,\n",
       "            2.2236e-05, 6.2448e-03],\n",
       "           [9.8800e-02, 3.3742e-02, 6.4992e-03,  ..., 9.9442e-06,\n",
       "            2.2236e-05, 6.2448e-03],\n",
       "           [9.8580e-03, 1.2881e-03, 3.6634e-03,  ..., 4.2522e-08,\n",
       "            4.3150e-08, 1.6871e-04],\n",
       "           ...,\n",
       "           [1.3638e-02, 4.9406e-04, 1.4518e-05,  ..., 1.7811e-07,\n",
       "            1.9837e-07, 4.0030e-04],\n",
       "           [8.4147e-02, 7.0304e-03, 9.5670e-05,  ..., 3.0237e-06,\n",
       "            2.0545e-06, 2.7143e-03],\n",
       "           [7.3480e-02, 2.2214e-03, 2.1817e-04,  ..., 1.1652e-05,\n",
       "            6.3662e-06, 3.2848e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.2861e-01, 1.9467e-02, 7.7696e-04,  ..., 2.8767e-06,\n",
       "            3.3763e-06, 2.2306e-03],\n",
       "           [1.2861e-01, 1.9467e-02, 7.7695e-04,  ..., 2.8767e-06,\n",
       "            3.3763e-06, 2.2306e-03],\n",
       "           [1.0105e-02, 4.7296e-04, 1.3769e-03,  ..., 1.8054e-06,\n",
       "            2.0375e-06, 1.1817e-04],\n",
       "           ...,\n",
       "           [1.4053e-01, 6.7282e-04, 7.7074e-05,  ..., 2.9806e-06,\n",
       "            3.9972e-06, 1.6329e-02],\n",
       "           [1.0989e-01, 7.6886e-03, 1.4122e-04,  ..., 6.7270e-06,\n",
       "            6.0537e-06, 8.8752e-03],\n",
       "           [4.2196e-02, 9.5544e-04, 3.2102e-04,  ..., 5.4092e-06,\n",
       "            1.1998e-05, 9.7277e-04]],\n",
       " \n",
       "          [[1.0347e-02, 8.1355e-02, 2.1036e-03,  ..., 1.6104e-06,\n",
       "            1.5895e-06, 4.3898e-03],\n",
       "           [1.0347e-02, 8.1355e-02, 2.1036e-03,  ..., 1.6104e-06,\n",
       "            1.5895e-06, 4.3898e-03],\n",
       "           [2.3850e-02, 1.5707e-03, 2.3358e-02,  ..., 2.8565e-07,\n",
       "            5.2866e-07, 6.7646e-04],\n",
       "           ...,\n",
       "           [6.8477e-03, 3.6138e-04, 2.6648e-06,  ..., 1.9913e-06,\n",
       "            6.9854e-07, 2.8755e-03],\n",
       "           [1.2894e-02, 1.2179e-02, 1.0122e-04,  ..., 3.6315e-06,\n",
       "            6.6165e-06, 3.9658e-03],\n",
       "           [1.2709e-02, 3.1118e-03, 8.9045e-04,  ..., 4.3198e-06,\n",
       "            7.3431e-06, 1.0161e-03]],\n",
       " \n",
       "          [[1.4007e-02, 1.2918e-01, 1.6701e-03,  ..., 3.2290e-05,\n",
       "            3.2519e-05, 1.6213e-02],\n",
       "           [1.4007e-02, 1.2918e-01, 1.6701e-03,  ..., 3.2290e-05,\n",
       "            3.2519e-05, 1.6213e-02],\n",
       "           [6.9275e-03, 6.9704e-03, 4.3710e-01,  ..., 3.4442e-07,\n",
       "            1.0622e-06, 4.8544e-03],\n",
       "           ...,\n",
       "           [5.0583e-03, 3.2058e-04, 7.4782e-07,  ..., 8.7918e-07,\n",
       "            6.6910e-07, 3.4744e-01],\n",
       "           [6.6550e-04, 2.3013e-02, 2.6539e-05,  ..., 1.2553e-05,\n",
       "            1.6057e-05, 6.7414e-03],\n",
       "           [6.2907e-04, 9.1646e-04, 4.5880e-03,  ..., 3.0618e-05,\n",
       "            4.8161e-05, 9.5715e-03]]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_attentions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
